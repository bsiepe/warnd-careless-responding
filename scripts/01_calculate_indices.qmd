---
title: "Careless Responding"
subtitle: "Calculating Indices"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
 - name: Add name(s) of person(s) responsible for respective file
   orcid: 0000-0002-9558-4648
   affiliations: to be added
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: true # run code?
params:
  rerun: false  # define parameter if all large analyses should be rerun
---

# Background
In this file, we will compute all indices of careless responding. We aim for the following structure: 
`external_id`, `counter`, `index`. There should be no implicit missingness. 

TODOS:

    - We should maybe return 0 for the within-assessment-SD instead of NA when there is no variability at all. 



We first load all relevant packages: 
```{r packages}
if(!require(pacman)) install.packages("pacman")
pacman::p_load("dplyr", "tidyr", "here", "ggplot2", "rrcov", "Hmisc", "sysfonts", "showtext", 
               "ggh4x", "cowplot", "gtsummary", "gt")  

source(here("scripts", "00_functions.R"))
  
set.seed(35032)  
```


We load the data and remove all non-EMA prompts. Insert your dataset in the subfolder `/data/` and load it here. 
```{r read-data}
#| eval: !expr params$rerun
data <- read.csv(here::here("data", "WARND_stage2_c1234_trainingset_before_30min_v2024_07_16.csv"))

data <- data |> 
  filter(!is.na(counter))
```

Relevant EMA items and their reaction times: 
```{r ema-itemss}
ema_items <-  c("sad_d", "stressed_d", "overwhelm_d", "nervous_d", "ruminate_d", "irritable_d", "cheerful_d", "motivated_d", "relaxed_d")
rt_ema_items <- paste0("rt_", ema_items)

```


# Data Preparation

We first subset the data for testing the functions on a smaller dataframe of 30 individuals:

```{r subset-data, eval = FALSE}
#| eval: !expr params$rerun
# select 30 individuals at random
ids_to_subset <- data |> 
  distinct(external_id) |> 
  slice_sample(n = 30) |> 
  pull(external_id)


subset_data <- data |> 
  filter(external_id %in% ids_to_subset)

saveRDS(subset_data, file = here::here("data", "indices_experiment_data.RDS"))

```

```{r, include = FALSE}
subset_data <- readRDS(here::here("data", "indices_experiment_data.RDS"))
```



# Analysis

Currently, this depends on `sd_within_assessment` being the first, to have no implicit missingness when using `left_join()` later below.
```{r}
# Define the functions and their parameters
index_functions <- list(
    sd_within_assessment = list(
    func = calc_within_assessment_sd,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  mahalanobis = list(
    func = calc_indicator_mahalanobis,
    args = list(items = ema_items)
  ),
  robust_pca = list(
    func = calc_indicator_rob_PCA_orthogonal_distance,
    args = list(items = ema_items)
  ),
  # leave out synonym for now
  # psychometric_syntonym = list(
  #   func = calc_psychometric_synonym_violations,
  #   args = list(
  #     items = ema_items,
  #     cor_threshold = 0.45,
  #     synonym_difference_threshold = 5
  #   )
  # ),
  psychometric_antonym = list(
    func = calc_psychometric_antonym_violations,
    args = list(
      items = ema_items,
      antonym_pair = c("stressed_d", "relaxed_d"),
      # cor_threshold = 0.45,
      antonym_maxvalue_threshold = 4
    )
  ),
  mean_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "mean")
  ),
  sd_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "sd")
  ),

  items_at_mode = list(
    func = calc_mode_percentage,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  longstring = list(
    func = calc_longstring_props, 
    args = list(
      items = ema_items, 
      rt_items = rt_ema_items
    )
  )
)


```


Calculate them all on the subset:

```{r calc-indices, message=FALSE}
#| eval: !expr params$rerun
# calculate all indices
indices <- index_functions |>
  purrr::map(
    .f = function(x) {
      do.call(x$func, c(list(data = subset_data), x$args))
    }
  )

# combine indices
df_indices <- indices |>
  purrr::reduce(
    .f = function(x, y) {
      x |>
        left_join(y, by = c("external_id", "counter"))
    }
  ) |> 
  dplyr::mutate(mode_pct = if_else(is.nan(mode_pct), NA, mode_pct)) |> 
  dplyr::rename(k_values = kValues)
```

For now, we also convert longstring and mode proportions to absolute numbers:
```{r}
#| eval: !expr params$rerun
df_indices <- df_indices |> 
  mutate(longstring_prop = as.integer(longstring_prop * length(ema_items)),
         mode_prop = as.integer(mode_pct * length(ema_items)))
```



## Full Analysis Individual Indices
We split the full data set into multiple chunks for easier computation: 
```{r}
#| eval: !expr params$rerun
# get list of ids
unique_ids <- data |> 
  distinct(external_id) |> 
  pull(external_id)

# create chunk labels
num_per_group <- 50
num_groups <- ceiling(length(unique_ids) / num_per_group)
group_labels <- rep(1:num_groups, each = num_per_group)[seq_along(unique_ids)]

# map external_ids to groups
id_groups <- data.frame(external_id = unique_ids, group = group_labels)
data_grouped <- data |> left_join(id_groups, by = "external_id")

# Split into a list of dataframes, one for each group
data_subsets <- split(data_grouped, data_grouped$group)

df_indices_list <- list()

time_before <- Sys.time()
# Process each chunk
for (i in seq(num_groups)) {
  cat("Processing chunk", i, "of", num_groups, "\n")
  
  # Get the current chunk
  chunk_data <- data_subsets[[i]]
  
  # Calculate indices for the current chunk
  indices <- index_functions |>
    purrr::map(
      .f = function(x) {
        do.call(x$func, c(list(data = chunk_data), x$args))
      }
    )
  
  # Combine indices for the current chunk
  chunk_indices <- indices |>
    purrr::reduce(
      .f = function(x, y) {
        x |>
          left_join(y, by = c("external_id", "counter"))
      }
    ) |>
    dplyr::mutate(mode_pct = if_else(is.nan(mode_pct), NA, mode_pct)) |>
    dplyr::rename(k_values = kValues)
  
  # Convert proportions to absolute numbers
  chunk_indices <- chunk_indices |>
    mutate(longstring_prop = as.integer(longstring_prop * length(ema_items)),
           mode_prop = as.integer(mode_pct * length(ema_items)))
  
  # Append the chunk results to the list
  df_indices_list[[i]] <- chunk_indices
}

time_diff <- Sys.time() - time_before
# ~30mins.

# Combine all chunk results
df_indices <- df_indices_list |>
  purrr::reduce(
    .f = function(x, y) {
      x |>
        bind_rows(y, .id = "chunk_id")
    }
  ) |> 
  select(-chunk_id) |> 
  arrange(external_id, counter)

# save results
saveRDS(df_indices, file = here::here("output", "df_indices.RDS"))
```

Read back in: 
```{r}
df_indices <- readRDS(file = here::here("output", "df_indices.RDS"))
```


For the robust PCA, we used the default settings of the `rrcov` R package with a maximum number of nine principal components, corresponding to the number of items used.

# Multiverse Indices

Here, we investigate different settings to detect careless responding. These concern: 
1. Different cutoffs and settings for individual indices
2. Different combination rules for multiple indices

We define a grid of different cutoffs and settings. These are sham values for now to check computation. 

```{r specifications}
sd_within_cutoff <- c(0.75, 1, 1.25)
mean_rt_cutoff <- c(1, 1.5, 2)
sd_rt_cutoff <- c(0.4, 0.6, 1)
longstring_cutoff <- c(6, 7, 8)
mode_cutoff <- c(5, 6)

specification_grid <- expand.grid(
  sd_within_cutoff, 
  mean_rt_cutoff, 
  sd_rt_cutoff,
  longstring_cutoff,
  mode_cutoff
)

colnames(specification_grid) <- 
  c(
  "sd_within_cutoff", 
  "mean_rt_cutoff", 
  "sd_rt_cutoff", 
  "longstring_cutoff", 
  "mode_cutoff"
)

# flagging response as careless
flag_careless <- function(df, cutoffs) {
  df |>
    mutate(
      flag_sd_within = assessment_sd < cutoffs$sd_within_cutoff,
      flag_mean_rt = mean_time_diff < cutoffs$mean_rt_cutoff,
      flag_sd_rt = sd_time_diff < cutoffs$sd_rt_cutoff,
      flag_longstring = longstring_prop > cutoffs$longstring_cutoff,
      flag_mode = mode_prop > cutoffs$mode_cutoff
    ) |>
    mutate(spec_id = paste0("spec_", cutoffs$sd_within_cutoff, "_",
                                      cutoffs$mean_rt_cutoff, "_",
                                      cutoffs$sd_rt_cutoff, "_",
                                      cutoffs$longstring_cutoff, "_",
                                      cutoffs$mode_cutoff))
}

results_list <- purrr::pmap(specification_grid, function(sd_within_cutoff, mean_rt_cutoff,
                                                  sd_rt_cutoff, longstring_cutoff, mode_cutoff) {
  cutoffs <- list(
    sd_within_cutoff = sd_within_cutoff,
    mean_rt_cutoff = mean_rt_cutoff,
    sd_rt_cutoff = sd_rt_cutoff,
    longstring_cutoff = longstring_cutoff,
    mode_cutoff = mode_cutoff
  )
  flag_careless(df_indices, cutoffs)
})

results_df <- bind_rows(results_list)

```

We then compute the multiverse by flagging each index at each observation as being indicative of careless responding or not:
```{r}

```




Apply different combination rules for multiple indices: 
```{r}

```



# Descriptives & Visualizations

## Summary statistics


Numbers in parentheses represent first and third quartile. 
```{r}
df_indices |>   
  mutate(k_values = as.factor(k_values), 
         # syno_violations = as.factor(syno_violations), 
         anto_violations = as.factor(anto_violations), 
         longstring_prop = as.factor(longstring_prop)) |> 
  gtsummary::tbl_summary(include = !c(external_id, counter),
                       missing_text = "(Missing)") |> 
  gtsummary::as_gt()
```


## Investigate very large values

There are antonym violation values that exceed 10. Investigate: 
```{r}
# large_anto <- df_indices |> 
#   filter(anto_violations > 10) |> 
#   slice_max(anto_violations) |> 
#   pull(external_id)
# 
# data |> 
#   filter(external_id == large_anto) |> 
#   View()

```




## Large grid plot with all indices


kValues seem to be useless, so we filter them out here. Also, we filter out extreme time differences.

We first create a plot for all integer/proportion indices: 
```{r}
# use color based on the Johnson palette from MetBrewer
fill_color <- "#132b69"


# A function factory for getting integer y-axis values.
# (https://gist.github.com/jhrcook/eb7b63cc57c683a6eb4986c4107a88ec)
integer_breaks <- function(n = 5, ...) {
    fxn <- function(x) {
        breaks <- floor(pretty(x, n, ...))
        names(breaks) <- attr(breaks, "labels")
        breaks
    }
    return(fxn)
}

integer_plot <- df_indices |>
  select(!c(external_id, counter, k_values)) |>
  select(anto_violations,
         # syno_violations,
         longstring_prop,
         mode_prop) |>
  rename(
    # "Synonym" = "syno_violations",
    "Antonym" = "anto_violations",
    "Longstring" = "longstring_prop",
    "Items at Mode" = "mode_prop"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value)) +
  geom_bar(fill = fill_color) +
  ggh4x::facet_wrap2(. ~ index, scales = "free_y", axes = "all") +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = 0:10) +
  theme_bs() +
  labs(x = "", y = "")
```

Alternatively, convert the plot to percentages: 
```{r}
integer_plot_pct <- df_indices |>
  select(!c(external_id, counter, k_values)) |>
  select(anto_violations,
         # syno_violations,
         longstring_prop,
         mode_prop) |>
  rename(
    # "Synonym" = "syno_violations",
    "Antonym" = "anto_violations",
    "Longstring" = "longstring_prop",
    "Items at Mode" = "mode_prop"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value, y = after_stat(prop))) +  # for percentage
  geom_bar(fill = fill_color) +
  ggh4x::facet_wrap2(. ~ index, scales = "free_y", axes = "all") +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent_format()) +
  scale_x_continuous(
    breaks = 0:11,
    expand = c(0, 0),
    limits = c(-.6, 11.1)
  ) +
  theme_bs() +
  labs(x = "", y = "Percentage", caption = "Removed some small values")
integer_plot_pct
```



Then create a plot for all continuous indices: 
```{r}
cont_plot <- df_indices |>
  select(!c(external_id, counter, k_values)) |>
  filter(sd_time_diff < 10) |>
  select(assessment_sd,
         mahalanobis_dist,
         robustpca_dist,
         mean_time_diff,
         sd_time_diff,
  ) |>
  rename(
    "Assessment SD" = "assessment_sd",
    "Mahalanobis" = "mahalanobis_dist",
    "Robust PCA" = "robustpca_dist",
    "Time Mean" = "mean_time_diff",
    "Time SD" = "sd_time_diff"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value)) +
  geom_density(fill = fill_color,
               color = fill_color,
               # use finer bandwidth
               adjust = 1 / 4) +
  ggh4x::facet_wrap2(. ~ index, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  theme_bs() +
  labs(x = "", y = "")

cont_plot
```

Combine the plots: 
```{r, warnings = FALSE}
combined_plot <- cowplot::plot_grid(integer_plot_pct, cont_plot, nrow = 2)

ggsave("indices_overview.pdf", 
       plot = combined_plot, 
       path = here::here("figures"),
       height = 10, 
       width = 8)
```





# Session Info

```{r session-info}
pander::pander(sessionInfo())

```

