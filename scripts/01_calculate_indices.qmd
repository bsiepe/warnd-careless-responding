---
title: "Careless Responding"
subtitle: "Calculating Indices"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
 - name: Add name(s) of person(s) responsible for respective file
   orcid: 0000-0002-9558-4648
   affiliations: to be added
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: true # run code?
params:
  rerun: false  # define parameter if all large analyses should be rerun
---

# Background
In this file, we will compute all indices of careless responding. We aim for the following structure: 
`external_id`, `counter`, `index`. There should be no implicit missingness. 


We first load all relevant packages: 
```{r packages}
if(!require(pacman))
  install.packages("pacman")
pacman::p_load(
  "here",
  "ggplot2",
  "rrcov",
  "Hmisc",
  "sysfonts",
  "showtext",
  "ggh4x",
  "cowplot",
  "gtsummary",
  "gt",
  "future",
  "furrr",
  "dplyr",
  "tidyr",
  "vegan",
  "tibble",
  "purrr",
  "flextable"
)


source(here("scripts", "00_functions.R"))

set.seed(35032)  
```


We load the data and remove all non-EMA prompts. Insert your dataset in the subfolder `/data/` and load it here. 
```{r read-data}
#| eval: !expr params$rerun
data <- read.csv(here::here("data", "WARND_stage2_c1234_trainingset_before_30min_v2024_07_16.csv"))

data <- data |> 
  filter(!is.na(counter))
```

Relevant EMA items and their reaction times: 
```{r ema-itemss}
ema_items <-  c("sad_d", "stressed_d", "overwhelm_d", "nervous_d", "ruminate_d", "irritable_d", "cheerful_d", "motivated_d", "relaxed_d")
rt_ema_items <- paste0("rt_", ema_items)

```


For later visualizations, we also load the results of different model-based approaches: 
```{r load-models}
lpa_res <- readRDS(here::here("output", "data_profiles_corrected.RDS"))
irt_res <- readRDS(here::here("output", "data_mixtureIRT.RDS"))
```

# Analysis

Currently, this depends on `sd_within_assessment` being the first, to have no implicit missingness when using `left_join()` later below.
```{r}
# Define the functions and their parameters
index_functions <- list(
    sd_within_assessment = list(
    func = calc_within_assessment_sd,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  mahalanobis = list(
    func = calc_indicator_mahalanobis,
    args = list(items = ema_items)
  ),
  robust_pca = list(
    func = calc_indicator_rob_PCA_orthogonal_distance,
    args = list(items = ema_items)
  ),
  psychometric_antonym = list(
    func = calc_psychometric_antonym_violations,
    args = list(
      items = ema_items,
      antonym_pair = c("stressed_d", "relaxed_d"),
      # cor_threshold = 0.45,
      antonym_maxvalue_threshold = 4
    )
  ),
  mean_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "mean")
  ),
  sd_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "sd")
  ),

  items_at_mode = list(
    func = calc_mode_percentage,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  longstring = list(
    func = calc_longstring_counts, 
    args = list(
      items = ema_items, 
      rt_items = rt_ema_items
    )
  )
)


```


## Full Analysis Individual Indices
We split the full data set into multiple chunks for easier computation: 
```{r}
#| eval: !expr params$rerun
# get list of ids
unique_ids <- data |> 
  distinct(external_id) |> 
  pull(external_id)

# create chunk labels
num_per_group <- 50
num_groups <- ceiling(length(unique_ids) / num_per_group)
group_labels <- rep(1:num_groups, each = num_per_group)[seq_along(unique_ids)]

# map external_ids to groups
id_groups <- data.frame(external_id = unique_ids, group = group_labels)
data_grouped <- data |> left_join(id_groups, by = "external_id")

# Split into a list of dataframes, one for each group
data_subsets <- split(data_grouped, data_grouped$group)

df_indices_list <- list()

time_before <- Sys.time()


# Process each chunk
for (i in seq(num_groups)) {
  cat("Processing chunk", i, "of", num_groups, "\n")

  # Get the current chunk
  chunk_data <- data_subsets[[i]]

  # Calculate indices for the current chunk
  indices <- index_functions |>
    purrr::map(
      .f = function(x) {
        do.call(x$func, c(list(data = chunk_data), x$args))
      }
    )

  # Combine indices for the current chunk
  chunk_indices <- indices |>
    purrr::reduce(
      .f = function(x, y) {
        x |>
          left_join(y, by = c("external_id", "counter"))
      }
    ) |>
    dplyr::mutate(mode_count = if_else(is.nan(mode_count), NA, mode_count)) |>
    dplyr::rename(k_values = kValues)

  # Convert proportions to absolute numbers
  chunk_indices <- chunk_indices |>
    mutate(longstring_count = as.integer(longstring_count * length(ema_items)),
           mode_count = as.integer(mode_count * length(ema_items)))

  # Append the chunk results to the list
  df_indices_list[[i]] <- chunk_indices
}

time_diff <- Sys.time() - time_before
# ~30mins.

# Combine all chunk results
df_indices <- df_indices_list |>
  purrr::reduce(
    .f = function(x, y) {
      x |>
        bind_rows(y, .id = "chunk_id")
    }
  ) |> 
  select(-chunk_id) |> 
  arrange(external_id, counter)

# save results
saveRDS(df_indices, file = here::here("output", "df_indices.RDS"))
```

Read back in: 
```{r}
#| label: load-indices
df_indices <- readRDS(file = here::here("output", "df_indices.RDS"))
```


For the robust PCA, we used the default settings of the `rrcov` R package with a maximum number of nine principal components, corresponding to the number of items used.

# Multiverse Indices

Here, we investigate different settings to detect careless responding. These concern: 
1. Different cutoffs and settings for individual indices
2. Different combination rules for multiple indices

We define a grid of different cutoffs and settings. 

```{r}
#| label: specifications
sd_within_cutoff <- c(0.75, 1, 1.25)
average_response_time_cutoff <- c(1, 1.5, 2)
sd_response_time_cutoff <- c(0.4, 0.6, 1)
longstring_cutoff <- c(6, 7, 8)
mode_cutoff <- c(5, 6)
anto_cutoff <- c(0)


specification_grid <- expand.grid(
  sd_within_cutoff, 
  average_response_time_cutoff, 
  sd_response_time_cutoff,
  longstring_cutoff,
  mode_cutoff,
  anto_cutoff
)

colnames(specification_grid) <- 
  c(
  "sd_within_cutoff", 
  "average_response_time_cutoff", 
  "sd_response_time_cutoff", 
  "longstring_cutoff", 
  "mode_cutoff",
  "anto_cutoff"
)



```

We then compute the multiverse by flagging each index at each observation as being indicative of careless responding or not:
```{r}
#| label: flag-careless
#| eval: !expr params$rerun
# flagging response as careless
flag_careless <- function(df, cutoffs) {
  df |>
    mutate(
      flag_sd_within = assessment_sd < cutoffs$sd_within_cutoff,
      flag_average_response_time = average_response_time < cutoffs$average_response_time_cutoff,
      flag_sd_response_time = sd_response_time < cutoffs$sd_response_time_cutoff,
      flag_longstring = longstring_count > cutoffs$longstring_cutoff,
      flag_mode = mode_count > cutoffs$mode_cutoff,
      flag_anto = anto_violations > cutoffs$anto_cutoff
    ) |>
    mutate(spec_id = paste0("spec_", "sd_", cutoffs$sd_within_cutoff, "_",
                                    "avg_rt_",  cutoffs$average_response_time_cutoff, "_",
                                    "sd_rt_",  cutoffs$sd_response_time_cutoff, "_",
                                    "long_",  cutoffs$longstring_cutoff, "_",
                                    "mode_",  cutoffs$mode_cutoff, "_",
                                    "anto_", cutoffs$anto_cutoff))
}

careless_results_list <- purrr::pmap(specification_grid, function(sd_within_cutoff, average_response_time_cutoff,
                                                  sd_response_time_cutoff, longstring_cutoff, mode_cutoff, anto_cutoff) {
  cutoffs <- list(
    sd_within_cutoff = sd_within_cutoff,
    average_response_time_cutoff = average_response_time_cutoff,
    sd_response_time_cutoff = sd_response_time_cutoff,
    longstring_cutoff = longstring_cutoff,
    mode_cutoff = mode_cutoff,
    anto_cutoff = anto_cutoff
  )
  flag_careless(df_indices, cutoffs)
})


saveRDS(careless_results_list, file = here::here("output", "careless_results_list.RDS"))
```

Reload the dataframe:
```{r}
#| label: reload-careless
careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))
df_careless <- bind_rows(careless_results_list)
```


We can then apply different combination rules for multiple indices. For the manuscript, we focus on our "main" specification, which is:

    - Within-assessment variability: 1.25
    - Longstring: 7/9
    - Mode: 5/9
    - Average response time: 2s
    - SD response time: 1s
    
```{r multiple-hurdles}
#| eval: !expr params$rerun
# select df of main specification
main_id <- specification_grid |>
  mutate(id = row_number()) |> 
  filter(
    sd_within_cutoff == 1.25 &
      average_response_time_cutoff == 2 &
      sd_response_time_cutoff == 1 &
      longstring_cutoff == 7 & mode_cutoff == 5
  ) |> 
    pull(id)


main_df <- careless_results_list[[main_id]]

# apply multiple hurdle approach
flag_df <- main_df |> 
  dplyr::select(external_id, counter, starts_with("flag")) |> 
  rowwise() |> 
  mutate(flag_sum = sum(flag_sd_within, flag_average_response_time, flag_sd_response_time, flag_longstring, flag_mode, flag_anto, na.rm = TRUE)) 


saveRDS(flag_df, file = here::here("output", "flag_df.RDS"))
```



## Checking impact of excluding careless responses

To understand the impact of different cutoffs for careless respondings, we can investigate how certain properties of the data or model results change before and after the exclusion of responses marked as careless. Here, inspired by Jaso et al. (2021), we do so by investigating how correlations between two items change. We pick "nervous" and "overwhelmed", as we expect these to correlate highly in non-careless responses.


### Multiverse Correlations
We iterate through the different specifications and compute the sum of flagged responses. We then select responses flagged as careless responses, and compute the correlation between the two negative affect items. 
```{r}
#| eval: !expr params$rerun
# make the careless results list smaller and easier to work with
careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags
flag_list <- lapply(careless_list, function(x){
    x |> 
    rowwise() |> 
    dplyr::mutate(flag_sum = sum(flag_sd_within, flag_average_response_time, flag_sd_response_time, flag_longstring, flag_mode, flag_anto, na.rm = TRUE)) |> 
    dplyr::select(external_id, counter, flag_sum)
})

# remove unnecessary data attributes
flag_list <- lapply(flag_list, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})
saveRDS(flag_list, file = here::here("output", "flag_list.RDS"))


```

Re-read the flag dataframe and filter careless responses for each specification, then compute the correlation of the two items:
```{r}
#| eval: !expr params$rerun
# select careless responses and filter them in original data, then compute correlation
flag_list <- readRDS(here::here("output", "flag_list.RDS"))


# compute correlation for each specification
compute_correlations <- function(flag_list, data) {
 imap(flag_list, function(flag_df, i) {
   cat("Processing iteration", i, "of", length(flag_list), "\n")
   
   excluded_ids_1plus <- flag_df |> 
     filter(flag_sum >= 1) |> 
     select(external_id, counter)
   
   excluded_ids_2plus <- flag_df |> 
     filter(flag_sum >= 2) |> 
     select(external_id, counter)
   
   excluded_data_1plus <- data |> 
     inner_join(excluded_ids_1plus, by = c("external_id", "counter"))
   
   excluded_data_2plus <- data |> 
     inner_join(excluded_ids_2plus, by = c("external_id", "counter"))
   
   list(
     correlation_1plus = cor(excluded_data_1plus$nervous_d, excluded_data_1plus$overwhelm_d, use = "pairwise.complete.obs"),
     n_excluded_1plus = nrow(excluded_data_1plus),
     correlation_2plus = cor(excluded_data_2plus$nervous_d, excluded_data_2plus$overwhelm_d, use = "pairwise.complete.obs"),
     n_excluded_2plus = nrow(excluded_data_2plus)
   )
 })
}

excluded_correlations <- compute_correlations(flag_list, data)

# combine to dataframe
df_cors <- bind_rows(excluded_correlations)

# add specification information
df_cors_spec <- cbind(df_cors, specification_grid)

saveRDS(df_cors_spec, here::here("output", "df_cors.RDS"))
```



Here is a visual summary of the results across different specifications in the form of a specification curve, first for the single-hurdle approach:

```{r}
df_cors_spec <- readRDS(here::here("output", "df_cors.RDS"))
# Combine plots
correlation_plot_single <- df_cors_spec |> 
    dplyr::arrange(correlation_1plus) |> 
    dplyr::mutate(iteration = dplyr::row_number()) |>
    ggplot(aes(x = iteration, y = correlation_1plus)) + 
    geom_point(size = 0.8) +
    theme_bs() +
    labs(x = "", y = "Correlation", subtitle = "Single-Hurdle Approach")
specification_plot <- plot_specification(df_cors_spec,
                                         type = "cor",
                                         hurdle = "single")

mv_plot_cors_single <- plot_grid(correlation_plot_single, specification_plot, 
          ncol = 1, rel_heights = c(1, 2), axis = "b",
          align = "h")

ggsave("mv_plot_cors_single.pdf", mv_plot_cors_single, device = "pdf", path = here::here("figures"),
       height = 9, width = 10)
```

Then for the multiple hurdle approach with two "hurdles":

```{r}
# Combine plots
correlation_plot_double <- df_cors_spec |> 
    dplyr::arrange(correlation_2plus) |> 
    dplyr::mutate(iteration = dplyr::row_number()) |>
    ggplot(aes(x = iteration, y = correlation_2plus)) + 
    geom_point(size = 0.8) +
    theme_bs() +
    labs(x = "", y = "Correlation", subtitle = "Duoble-Hurdle Approach")
specification_plot <- plot_specification(df_cors_spec,
                                         type = "cor",
                                         hurdle = "double")

mv_plot_cors_double <- plot_grid(
  correlation_plot_double,
  specification_plot,
  ncol = 1,
  rel_heights = c(1, 2),
  axis = "b",
  align = "h"
)

ggsave("mv_plot_cors_double.pdf", mv_plot_cors_double, device = "pdf", path = here::here("figures"),
       height = 9, width = 10)

```


### Multiverse Similarity 
Instead of checking the correlation between affect items, we can instead check the overlap between carelessness flags based on the indices and the model-based indices. 

```{r}
#| eval: !expr params$rerun
flag_list <- readRDS(here::here("output", "flag_list.RDS"))
# compute similarity in each combination 
compute_similarities <- function(flag_list, lpa_res, irt_res) {
 imap(flag_list, function(flag_df, i) {
   cat("Processing iteration", i, "of", length(flag_list), "\n")
   
   df_indices_models <- lpa_res |> 
     left_join(irt_res, by = c("external_id", "counter")) |> 
     left_join(flag_df, by = c("external_id", "counter")) |> 
     mutate(across(where(is.logical), ~ as.integer(.))) |> 
     # convert to binary 
     mutate(flag_lpa = ifelse(LPA_profile != 2, 1, 0),
            flag_irt = ifelse(mixtureIRT == 1, 0, 1),
            flag_sum1 = ifelse(flag_sum == 0, 0, 1),
            flag_sum2 = ifelse(flag_sum <2, 0, 1)) |> 
     select(contains("flag"))
   
   # computing jaccard similarity
   jaccard_matrix <- vegan::vegdist(t(df_indices_models), na.rm = TRUE, method = "jaccard", binary = TRUE)
   jaccard_matrix <- as.matrix(jaccard_matrix)
   
   # convert dissimilarity to similarity
   jaccard_sim_matrix <- 1 - jaccard_matrix
   diag(jaccard_sim_matrix) <- 1
   
   list(
     n_careless_sum1 = sum(df_indices_models$flag_sum1 != 0, na.rm = TRUE),
     n_careless_sum2 = sum(df_indices_models$flag_sum2 != 0, na.rm = TRUE),
     n_careless_lpa = sum(df_indices_models$flag_lpa, na.rm = TRUE),
     n_careless_irt = sum(df_indices_models$flag_irt, na.rm = TRUE),
     jaccard_similarity1_lpa = jaccard_sim_matrix[["flag_sum1", "flag_lpa"]],
     jaccard_similarity1_irt = jaccard_sim_matrix[["flag_sum1", "flag_irt"]],
     jaccard_similarity2_lpa = jaccard_sim_matrix[["flag_sum2", "flag_lpa"]],
     jaccard_similarity2_irt = jaccard_sim_matrix[["flag_sum2", "flag_irt"]]    
   )
 })
}

jaccard_results <- compute_similarities(flag_list, lpa_res, irt_res)
names(jaccard_results) <- names(flag_list)

# format properly and add specification info
df_jaccard <- bind_rows(jaccard_results)
df_jaccard_spec <- cbind(df_jaccard, specification_grid)


saveRDS(df_jaccard_spec, file = here::here("output", "df_jaccard_spec.RDS"))
```


Visualize the results for both models and the single and double hurdle approach: 
```{r}
df_jaccard_spec <- readRDS(here::here("output", "df_jaccard_spec.RDS"))
hurdles <- c("single", "double")
models <- c("lpa", "irt")
type <- "similarity"

# plots for similarities
for(model in models) {
  for(hurdle in hurdles) {
    sim_plot <- plot_similarity(df_jaccard_spec, model, hurdle = hurdle)
    spec_plot <- plot_specification(df_jaccard_spec, type = "similarity", model = model, hurdle = hurdle)
    
    mv_plot <- plot_grid(
      sim_plot,
      spec_plot,
      ncol = 1,
      rel_heights = c(1, 2),
      align = "h",
      axis = "b"
    )
    
    ggsave(
      paste0("mv_plot_similarity_", model, "_", hurdle, ".pdf"),
      mv_plot,
      device = "pdf",
      path = here::here("figures"),
      height = 10,
      width = 10
    )
  }
}
```


## Multiverse descriptives

Instead of computing correlations or similarities, we can simply calculate the number of responses flagged as careless for each combination of indices: 

```{r}
#| label: multiverse-careless-pct
#| eval: !expr params$rerun
careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags
flag_list <- lapply(careless_list, function(x){
    x |> 
    rowwise() |> 
    dplyr::mutate(flag_sum = sum(flag_sd_within, flag_average_response_time, flag_sd_response_time, flag_longstring, flag_mode, flag_anto, na.rm = TRUE)) |>
    mutate(flag_sum1 = ifelse(flag_sum == 0, 0, 1),
            flag_sum2 = ifelse(flag_sum <2, 0, 1))
})

# remove unnecessary data attributes
flag_list <- lapply(flag_list, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})

rm(careless_list)

# for each list entry, calculate careless responding proportions
# normed by the number of responses in our main df
flag_df <- readRDS(here::here("output", "flag_df.RDS"))

total_obs <- nrow(flag_df)
non_missing_obs <- flag_df |> 
  filter(!is.na(flag_sd_within)) |> 
  nrow()


flag_summary <- lapply(flag_list, function(x){
  x |> 
  ungroup() |> 
  select(!c(external_id, counter, flag_sum)) |> 
  summarize(across(everything(), 
                   ~sum(., na.rm = TRUE))) |> 
  mutate(across(everything(), 
                ~ round(./ non_missing_obs * 100, 3))) 
})

rm(flag_list)

# attach specification information
flag_multiverse <- do.call(rbind, flag_summary) |> 
  bind_cols(specification_grid)


saveRDS(flag_multiverse, here::here("output", "flag_multiverse.RDS"))
```

Display:
```{r}
#| label: multiverse-flag-table

flag_multiverse |> 
  knitr::kable()
```



# Descriptives & Visualizations

## Summary statistics
Numbers in parentheses represent first and third quartiles. 
```{r}
df_indices |>   
  mutate(k_values = as.factor(k_values), 
         anto_violations = as.factor(anto_violations), 
         longstring_count = as.factor(longstring_count)) |> 
  gtsummary::tbl_summary(include = !c(external_id, counter),
                       missing_text = "(Missing)") |> 
  gtsummary::as_gt()
```


## Main cutoffs

Above, we defined the main default cutoffs that we use for the paper. We now check how many responses are marked as careless for each of the indices (in percent of all responses). Please note that we obtain slightly different percentages (second decimal difference) to the results in @sec-overview-indices. This difference occurs due to the way in which missing values are recorded. Here, we calculate them based on the within-assessment SD - below, we calculate them based on the missings for each individual careless response flag. For the overview in the manuscript, we keep it as it is here. 
```{r}
flag_df <- readRDS(here::here("output", "flag_df.RDS"))

total_obs <- nrow(flag_df)
non_missing_obs <- flag_df |> 
  filter(!is.na(flag_sd_within)) |> 
  nrow()

flag_df |> 
  ungroup() |> 
  select(!c(external_id, counter, flag_sum)) |> 
  summarize(across(everything(), 
                   ~sum(., na.rm = TRUE))) |> 
  mutate(across(everything(), 
                ~ round(./ non_missing_obs * 100, 3))) |> 
  knitr::kable()

```



What happens when using the multiple-hurdle approach with these indices? We again check how many responses are marked as careless for each of the approaches (in percent of all responses). 
```{r}
df_jaccard_spec |> 
    filter(
    sd_within_cutoff == 1.25 &
      average_response_time_cutoff == 2 &
      sd_response_time_cutoff == 1 &
      longstring_cutoff == 7 & mode_cutoff == 5
  ) |> 
  mutate(across(contains("n_"),
                ~ round(./ non_missing_obs * 100, 2))) |> 
  rename(
    "Prop. Careless Single Hurdle" = n_careless_sum1, 
    "Prop. Careless Double Hurdle" = n_careless_sum2
  ) |> 
  knitr::kable()
```

How would these change if we slightly changed one cutoff, e.g., the SD response time cutoff to 0.6?
```{r}
#| label: change-sd-cutoff
df_jaccard_spec |> 
    filter(
    sd_within_cutoff == 1.25 &
      average_response_time_cutoff == 2 &
      sd_response_time_cutoff == 1 &
      longstring_cutoff == 7 & mode_cutoff == 6
  ) |> 
  mutate(across(contains("n_"),
                ~ round(./ non_missing_obs * 100, 2))) |> 
  rename(
    "Prop. Careless Single Hurdle" = n_careless_sum1, 
    "Prop. Careless Double Hurdle" = n_careless_sum2
  ) |> 
  knitr::kable()
```



## Overview over all single-index cutoffs {#sec-overview-indices}

Here, we provide an overview over the proportion of responses flagged as careless for each individual index cutoff:
```{r}
#| label: index-overview-table
# cutoff values for each index
cutoffs <- list(
  sd_within = c(0.75, 1, 1.25),
  avg_rt = c(1, 1.5, 2),
  sd_rt = c(0.4, 0.6, 1),
  longstring = c(6, 7, 8),
  mode = c(5, 6),
  anto = c(0)
)

# calculate proportions for each index
results <- map_dfr(names(cutoffs), function(index_name) {
  cutoff_values <- cutoffs[[index_name]]
  
  # determine the column name and comparison operator
  col_name <- case_when(
    index_name == "sd_within" ~ "assessment_sd",
    index_name == "avg_rt" ~ "average_response_time", 
    index_name == "sd_rt" ~ "sd_response_time",
    index_name == "longstring" ~ "longstring_count",
    index_name == "mode" ~ "mode_count",
    index_name == "anto" ~ "anto_violations"
  )
  
  # calculate proportions for each cutoff
  props <- map_dbl(cutoff_values, function(cutoff) {
    valid_n <- sum(!is.na(df_indices[[col_name]]))
    if (index_name %in% c("longstring", "mode", "anto")) {
      sum(df_indices[[col_name]] > cutoff, na.rm = TRUE) / valid_n
    } else {
      sum(df_indices[[col_name]] < cutoff, na.rm = TRUE) / valid_n
    }
  })
  
  # create a tibble with proper column names
  result <- tibble(index = index_name)
  for (i in seq_along(cutoff_values)) {
    col_name <- paste0(index_name, "_", cutoff_values[i])
    result[[col_name]] <- props[i]
  }
  
  result
})

clean_table <- results |>
  rowwise() |>
  mutate(
    cutoff_values = case_when(
      index == "sd_within" ~ list(c(sd_within_0.75, sd_within_1, sd_within_1.25)),
      index == "avg_rt" ~ list(c(avg_rt_1, avg_rt_1.5, avg_rt_2)),
      index == "sd_rt" ~ list(c(sd_rt_0.4, sd_rt_0.6, sd_rt_1)),
      index == "longstring" ~ list(c(longstring_6, longstring_7, longstring_8)),
      index == "mode" ~ list(c(mode_5, mode_6)),
      index == "anto" ~ list(c(anto_0))
    ),
    cutoff_labels = case_when(
      index == "sd_within" ~ list(c("0.75", "1.0", "1.25")),
      index == "avg_rt" ~ list(c("1.0", "1.5", "2.0")),
      index == "sd_rt" ~ list(c("0.4", "0.6", "1.0")),
      index == "longstring" ~ list(c("6", "7", "8")),
      index == "mode" ~ list(c("5", "6")),
      index == "anto" ~ list(c("0"))
    )
  ) |>
  unnest_longer(c(cutoff_values, cutoff_labels)) |>
  select(index, cutoff_labels, cutoff_values) |>
  mutate(cutoff_values = paste0(round(cutoff_values, 4) * 100, " %")) |> 
  pivot_wider(names_from = cutoff_labels, values_from = cutoff_values) |>
  ungroup()

# create flextable
ft <- clean_table |>
  flextable() |>
  set_header_labels(index = "Index") |>
  theme_vanilla() |>
  autofit() |>
  align(align = "center", part = "all") 

ft
  
```




## Large grid plot with all indices


kValues seem to be useless, so we filter them out here. Also, we filter out extreme time differences.

We first create a plot for all integer/proportion indices: 
```{r}
# use color based on the Johnson palette from MetBrewer
fill_color <- "#132b69"


# A function factory for getting integer y-axis values.
# (https://gist.github.com/jhrcook/eb7b63cc57c683a6eb4986c4107a88ec)
integer_breaks <- function(n = 5, ...) {
    fxn <- function(x) {
        breaks <- floor(pretty(x, n, ...))
        names(breaks) <- attr(breaks, "labels")
        breaks
    }
    return(fxn)
}

integer_plot <- df_indices |>
  dplyr::select(!c(external_id, counter, k_values)) |>
  dplyr::select(anto_violations,
         # syno_violations,
         longstring_count,
         mode_count) |>
  rename(
    # "Synonym" = "syno_violations",
    "Antonym" = "anto_violations",
    "Longstring" = "longstring_count",
    "Items at Mode" = "mode_count"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value)) +
  geom_bar(fill = fill_color) +
  ggh4x::facet_wrap2(. ~ index, scales = "free_y", axes = "all") +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = 0:10) +
  theme_bs() +
  labs(x = "", y = "")
```

Alternatively, convert the plot to percentages: 
```{r}
integer_plot_pct <- df_indices |>
  dplyr::select(!c(external_id, counter, k_values)) |>
  dplyr::select(anto_violations,
         # syno_violations,
         longstring_count,
         mode_count) |>
  rename(
    # "Synonym" = "syno_violations",
    "Antonym" = "anto_violations",
    "Longstring" = "longstring_count",
    "Items at Mode" = "mode_count"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value, y = after_stat(prop))) +  # for percentage
  geom_bar(fill = fill_color) +
  ggh4x::facet_wrap2(. ~ index, scales = "free_y", axes = "all") +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent_format()) +
  scale_x_continuous(
    breaks = 0:11,
    expand = c(0, 0),
    limits = c(-.6, 11.1)
  ) +
  theme_bs() +
  labs(x = "", y = "Percentage")
integer_plot_pct
```



Then create a plot for all continuous indices: 
```{r}
cont_plot <- df_indices |>
  dplyr::select(!c(external_id, counter, k_values)) |>
  filter(sd_response_time < 10) |>
  dplyr::select(assessment_sd,
         mahalanobis_dist,
         robustpca_dist,
         average_response_time,
         sd_response_time,
  ) |>
  rename(
    "Assessment SD" = "assessment_sd",
    "Mahalanobis" = "mahalanobis_dist",
    "Robust PCA" = "robustpca_dist",
    "Average RT" = "average_response_time",
    "SD RT" = "sd_response_time"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value)) +
  geom_density(fill = fill_color,
               color = fill_color,
               # use finer bandwidth
               adjust = 1 / 4) +
  ggh4x::facet_wrap2(. ~ index, scales = "free") +
  scale_y_continuous(expand = c(0, 0), labels = scales::label_percent()) +
  theme_bs() +
  labs(x = "", y = "")

cont_plot
```

Combine the plots: 
```{r, warnings = FALSE}
combined_plot <- cowplot::plot_grid(integer_plot_pct, cont_plot, nrow = 2)

ggsave("indices_overview.pdf", 
       plot = combined_plot, 
       path = here::here("figures"),
       height = 10, 
       width = 8)
```



# Similarity Indices and Models

We attach the model results to the indices. We classify everyone with an LPA profile 1 or 2, and a mixture IRT class 1 as attentive. 
```{r}
#| label: attach-model
df_indices_models <- lpa_res |> 
  left_join(irt_res, by = c("external_id", "counter")) |> 
  left_join(flag_df, by = c("external_id", "counter")) |> 
    mutate(across(where(is.logical), ~ as.integer(.))) |> 
    mutate(flag_lpa = ifelse(LPA_profile != 2, 1, 0),
           flag_irt = ifelse(mixtureIRT == 1, 0, 1)) |> 
   select(contains("flag")) |> 
  # recode to obtain single hurdle and multiple hurdle approach
  mutate(
    single_hurdle = if_else(flag_sum == 0, 0, 1),
    double_hurdle = if_else(flag_sum < 2, 0, 1)
  ) |> 
  # then remove raw sum
  select(!flag_sum)
  
```


## Large grid plot with indicators and model results


### LPA and Mixture IRT
Create distribution plot for the LPA and mixture IRT: 

```{r}
#| label: model-distributions
lpa_prop_df <- lpa_res |> 
  left_join(irt_res, by = c("external_id", "counter")) |>
  left_join(flag_df, by = c("external_id", "counter")) |> 
  mutate(across(where(is.logical), ~ as.integer(.))) |> 
  count(LPA_profile) |> 
  mutate(percent = n / sum(n))

# plot LPA results
lpa_dist_plot <- lpa_prop_df |> 
  ggplot(aes(x = as.factor(LPA_profile), y = percent, fill = as.factor(LPA_profile))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_viridis_d() +
  scale_y_continuous(limits = c(0, .45), expand = c(0, 0), labels = scales::percent_format()) +
  theme_bs() +
  labs(x = "Profile Membership", 
       y = "",
       title = "Latent Profile")

lpa_dist_plot  

ggsave("lpa_descriptive_plot.pdf", 
       plot = lpa_dist_plot, 
       path = here::here("figures"),
       height = 10, 
       width = 8)
```

Plot mixture IRT class membership overall:
```{r}
irt_prop_df <- lpa_res |> 
  left_join(irt_res, by = c("external_id", "counter")) |>
  left_join(flag_df, by = c("external_id", "counter")) |> 
  mutate(across(where(is.logical), ~ as.integer(.))) |> 
  count(mixtureIRT) |> 
  mutate(percent = n / sum(n)) |> 
  filter(!is.na(mixtureIRT))

# plot irt results
irt_dist_plot <- irt_prop_df |>
  ggplot(aes(
    x = as.factor(mixtureIRT),
    y = percent,
    fill = as.factor(mixtureIRT)
  )) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_viridis_d() +
  scale_y_continuous(
    limits = c(0, 1.05),
    expand = c(0, 0),
    labels = scales::percent_format()
  ) +
  theme_bs() +
  labs(x = "Mixture IRT Class", y = "Percentage of Participants")

irt_dist_plot  

ggsave("irt_descriptive_plot.pdf", 
       plot = irt_dist_plot, 
       path = here::here("figures"),
       height = 10, 
       width = 8)
```

Plot mixture IRT class membership by calculating how often each individual was classified as being in the inattentive state:

```{r}
irt_prop_df <- lpa_res |> 
  left_join(irt_res, by = c("external_id", "counter")) |>
  left_join(flag_df, by = c("external_id", "counter")) |> 
  mutate(across(where(is.logical), ~ as.integer(.))) |> 
  filter(!is.na(mixtureIRT)) |> 
  group_by(external_id) |> 
  summarise(n_careless = sum(mixtureIRT == 2, na.rm = TRUE)) |> 
  ungroup()

# summarize how many participants had X number of careless time points
irt_summary_df <- irt_prop_df |> 
  count(n_careless) |> 
  mutate(percent = n / sum(n))

# plot
irt_dist_plot_timepoints <- irt_summary_df |> 
  ggplot(aes(x = n_careless, y = percent)) +
  geom_bar(stat = "identity", show.legend = FALSE, fill = fill_color) +
  scale_y_continuous(
    limits = c(0, .25),
    expand = c(0, 0),
    labels = scales::percent_format()
  ) +
  scale_x_continuous(
    breaks = scales::pretty_breaks(),
    expand = c(0, 0)
  ) +
  theme_bs() +
  labs(
    x = "Number of Careless Time Points",
    # y = "Percentage of Participants"
  )

irt_dist_plot_timepoints

```

Alternatively, we can bin the number of careless time points:
```{r}
# bin careless counts
irt_binned_df <- irt_prop_df |> 
  mutate(
    careless_bin = cut(
      n_careless,
      breaks = c(-Inf, 0, 5, 10, 20, 50, Inf),
      labels = c("0", "1‚Äì5", "6‚Äì10", "11‚Äì20", "21‚Äì50", "51+"),
      right = TRUE
    )
  )

# summarize number of participants in each bin
irt_summary_df <- irt_binned_df |> 
  count(careless_bin) |> 
  mutate(percent = n / sum(n))

# plot
irt_dist_plot_binned <- irt_summary_df |> 
  ggplot(aes(x = careless_bin, y = percent)) +
  geom_bar(stat = "identity", show.legend = FALSE, fill = fill_color) +
  scale_y_continuous(
    limits = c(0, .45),
    expand = c(0, 0),
    labels = scales::percent_format()
  ) +
  # scale_fill_viridis_d() +
  theme_bs() +
  labs(
    x = "N¬∞ of Careless Time Points",
    y = "",
    title = "Mixture IRT"
  )
  # theme(
  #   axis.text.x = element_text(size = 15)
  # )

irt_dist_plot_binned
```


### Combined with indices plots

Now combine these plots with all individual indices plots:
```{r giga-plot}
# create individual plots for each index
df_integer_long <- df_indices |>
  select(!c(external_id, counter, k_values)) |>
  select(
    anto_violations,
    longstring_count,
    mode_count
  ) |>
  rename(
    "Antonym violation" = "anto_violations",
    "Long string count" = "longstring_count",
    "Items that fall at the mode" = "mode_count"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value)) |>
  # recode antonym violation 
  mutate(
    value_label = case_when(
      index == "Antonym violation" & value == 0 ~ "no",
      index == "Antonym violation" & value == 1 ~ "yes",
      TRUE ~ as.character(value)
    ),
    # create labels for display
    value_numeric = value,
    value_display = if_else(index == "Antonym violation", value_label, as.character(value))
  )

# create list of individual bar plots
integer_plot_list <- df_integer_long |>
  group_split(index) |>
  map(~ {
    index_label <- unique(.x$index)
    
    
    p <- ggplot(.x, aes(x = value_numeric, y = after_stat(prop))) +
      geom_bar(fill = fill_color) +
      scale_y_continuous(expand = c(0, 0), labels = scales::percent_format()) +
      theme_bs() +
      labs(x = "", y = "") +
      ggtitle(index_label)
    
    # customize x-axis for the antonym violation (binary) plot
    if (index_label == "Antonym violation") {
      p <- p + scale_x_continuous(
        breaks = c(0, 1),
        labels = c("no", "yes"), 
        limits = c(-0.5, 1.5),
        expand = c(0, 0)
      )
    } else {
      p <- p + scale_x_continuous(
        breaks = 0:11,
        limits = c(-0.6, 9.1),
        expand = c(0, 0)
      )
    }
    p
  })

# create list of individual density plots
df_cont_long <- df_indices |>
  select(!c(external_id, counter, k_values)) |>
  filter(sd_response_time < 10) |>
  select(
    assessment_sd,
    mahalanobis_dist,
    robustpca_dist,
    average_response_time,
    sd_response_time
  ) |>
  rename(
    "Within-assessment response SD" = "assessment_sd",
    "Mahalanobis distance" = "mahalanobis_dist",
    "Robust PCA distance" = "robustpca_dist",
    "Average time-per-item" = "average_response_time",
    "SD time-per-item" = "sd_response_time"
  ) |>
  pivot_longer(cols = everything(), names_to = "index") |>
  filter(!is.na(value))


cont_plot_list <- df_cont_long |>
  group_split(index) |>
  map(~ {
    # scaled densities
    ggplot(.x, aes(x = value, ..scaled..)) +
      geom_density(fill = fill_color, color = fill_color, adjust = 1 / 4) +
      scale_y_continuous(expand = c(0, 0)) +
      theme_bs() +
      labs(x = "", y = "") +
      ggtitle(unique(.x$index))
  })

# combine into grid
plot_grid_list <- c(integer_plot_list, cont_plot_list, list(lpa_dist_plot, irt_dist_plot_binned))

combined_plot_all <- cowplot::plot_grid(
  plotlist = plot_grid_list,
  ncol = 2,  
  align = "hv"
)

ggsave("indices_models_overview.pdf", 
       plot = combined_plot_all, 
       path = here::here("figures"),
       height = 16, 
       width = 11)
```


Alternatively, we can add the same plot without the two models:

```{r}
#| label: distribution-plot-indices-only
#| warning: false
plot_grid_list <- c(integer_plot_list, cont_plot_list)

# reorder based on order in methods section
plot_grid_list <- plot_grid_list[c(3, 2, 8, 5, 6, 1, 4, 7)]

combined_plot_indices <- cowplot::plot_grid(
  plotlist = plot_grid_list,
  ncol = 2,  
  align = "hv"
)

ggsave("indices_overview_twocol.pdf", 
       plot = combined_plot_indices, 
       path = here::here("figures"),
       height = 14, 
       width = 11)
```



## Additional plot with number of careless observations

In one plot, we show the number of careless observations with a single and double-hurdle approach and the two models. 

```{r}
#| label: careless-response-number-figure

# create method labels
method_labels <- c(
  "flag_lpa" = "Latent Profile Analysis",
  "flag_irt" = "Mixture IRT", 
  "single_hurdle" = "Single Hurdle",
  "double_hurdle" = "Double Hurdle"
)

model_hurdle_counts <- lpa_res |> 
  left_join(irt_res, by = c("external_id", "counter")) |> 
  left_join(flag_df, by = c("external_id", "counter")) |> 
    mutate(across(where(is.logical), ~ as.integer(.))) |> 
    mutate(flag_lpa = ifelse(LPA_profile != 2, 1, 0),
           flag_irt = ifelse(mixtureIRT == 1, 0, 1)) |> 
  # recode to obtain single hurdle and multiple hurdle approach
  mutate(
    single_hurdle = if_else(flag_sum == 0, 0, 1),
    double_hurdle = if_else(flag_sum < 2, 0, 1)
  ) |> 
  select(external_id, flag_lpa, flag_irt, single_hurdle, double_hurdle) |> group_by(external_id) |> 
  summarize(across(everything(), sum)) |> 
  select(!external_id) |> 
    pivot_longer(everything(), names_to = "method", values_to = "count") |>
  filter(!is.na(count)) |> 
  mutate(method = factor(method, levels = names(method_labels)))



# create bins with same breaks for all methods
binned_df <- model_hurdle_counts |>
  mutate(
    careless_bin = cut(
      count,
      breaks = c(-Inf, 0, 10, 25, 50, 100, 200, Inf),
      labels = c("0", "1‚Äì10", "11‚Äì25", "26‚Äì50", "51‚Äì100", "101‚Äì200", "201+"),
      right = TRUE
    )
  )

# summarize proportions by method and bin
summary_df <- binned_df |>
  group_by(method, careless_bin) |>
  summarise(n = n(), .groups = "drop") |>
  group_by(method) |>
  mutate(percent = n / sum(n)) |>
  ungroup()



# plot
careless_dist_plot <- summary_df |>
  ggplot(aes(x = careless_bin, y = percent)) +
  geom_bar(stat = "identity",
           show.legend = FALSE,
           fill = fill_color) +
  ggh4x::facet_wrap2( ~ method, 
                      labeller = labeller(method = method_labels),
                      axes = "all")  +
  scale_y_continuous(
    limits = c(0, 0.75),
    expand = c(0, 0),
    labels = scales::percent_format()
  ) +
  theme_bs() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(size = 10, face = "bold")
  ) +
  labs(x = "Number of Careless Responses", y = "Proportion of Participants")

careless_dist_plot


ggsave("careless_count_plot.pdf", 
       plot = careless_dist_plot, 
       path = here::here("figures"),
       height = 9, 
       width = 11)
```




## Jaccard Similarity of Indices and Model Results

We can visualize the classification of careless responding based on indices and and model results with a Jaccard similarity heatmap. 

```{r jaccard-similarity}
jaccard_matrix <- vegan::vegdist(
  t(df_indices_models),
  na.rm = TRUE,
  method = "jaccard",
  binary = TRUE
)
jaccard_matrix <- as.matrix(jaccard_matrix)

# convert dissimilarity to similarity
jaccard_sim_matrix <- 1-jaccard_matrix

# self-similarity
diag(jaccard_sim_matrix) <- 1

column_order <- c("Longstring", "Mode", "SD within", "Antonym", "Avg. RT", "SD RT", "Single", "Double", "LPA", "IRT")

similarity_plot <- jaccard_sim_matrix |>
  as.data.frame() |>
  rownames_to_column(var = "Var1") |>
  pivot_longer(-Var1, names_to = "Var2", values_to = "Similarity") |>
  mutate(
    Var1 = case_when(
      Var1 == "flag_sd_within" ~ "SD within",
      Var1 == "flag_average_response_time" ~ "Avg. RT",
      Var1 == "flag_sd_response_time" ~ "SD RT",
      Var1 == "flag_longstring" ~ "Longstring",
      Var1 == "flag_mode" ~ "Mode",
      Var1 == "flag_anto" ~ "Antonym",
      Var1 == "flag_lpa" ~ "LPA",
      Var1 == "flag_irt" ~ "IRT",
      Var1 == "single_hurdle" ~ "Single",
      Var1 == "double_hurdle" ~ "Double",
      TRUE ~ Var1
    ),
    Var2 = case_when(
      Var2 == "flag_sd_within" ~ "SD within",
      Var2 == "flag_average_response_time" ~ "Avg. RT",
      Var2 == "flag_sd_response_time" ~ "SD RT",
      Var2 == "flag_longstring" ~ "Longstring",
      Var2 == "flag_mode" ~ "Mode",
      Var2 == "flag_anto" ~ "Antonym",
      Var2 == "flag_lpa" ~ "LPA",
      Var2 == "flag_irt" ~ "IRT",
      Var2 == "single_hurdle" ~ "Single",
      Var2 == "double_hurdle" ~ "Double",
      TRUE ~ Var2
    )) |> 
  mutate(
    Var1 = factor(Var1, levels = column_order),
    Var2 = factor(Var2, levels = rev(column_order)),
    text_color = if_else(Var1 == Var2, "white", "black")
  ) |>
  ggplot(aes(x = Var1, y = Var2, fill = Similarity)) +
  geom_tile(color = "white") +
  scale_fill_gradient(
    low = "white",
    high = fill_color,
    limit = c(0, 1),
    space = "Lab",
    name = "Jaccard\nSimilarity"
  ) +
  theme_bs() + 
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    legend.justification = c(1, 0),
    legend.position = "none"
  ) +
  coord_fixed() +
  geom_text(aes(label = round(Similarity, 2), color = text_color), size = 3.8) +
  scale_color_identity()

similarity_plot

ggsave("similarity_plot.pdf", similarity_plot, path = here::here("figures"),
       width = 7, height = 5)

```



# Session Info

```{r session-info}
pander::pander(sessionInfo())

```

