---
title: "Online Supplement 1: Indices"
subtitle: "For the Manuscript: Identifying Careless Responding in Ecological Momentary Assessment: Inconsistent Signals from Different Detection Methods in the WARN-D Data"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 8
    fig-height: 5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: true # run code?
params:
  rerun: false  # define parameter if all large analyses should be rerun
---

# Background
This document provides a workflow for computing all indices of careless responding used in our manuscript:
Identifying Careless Responding in Ecological Momentary Assessment: Inconsistent Signals from Different Detection Methods in the WARN-D Data.

We will:

-  Calculate a comprehensive set of indices for detecting careless responding.
-  Summarize and interpret the results.

Visualizations of our results are provided in `01_visualize_indices.qmd`.


We first load all relevant packages: 
```{r packages}
if(!require(pacman))
  install.packages("pacman")
pacman::p_load(
  "here",
  "ggplot2",
  "rrcov",
  "Hmisc",
  "sysfonts",
  "showtext",
  "ggh4x",
  "cowplot",
  "gtsummary",
  "gt",
  "future",
  "furrr",
  "dplyr",
  "tidyr",
  "vegan",
  "tibble",
  "purrr",
  "flextable",
  "patchwork"
)

source(here("scripts", "00_functions.R"))

# use color based on the Johnson palette from MetBrewer
fill_color <- "#132b69"

set.seed(35032)  
```


We load the data and remove all non-EMA prompts. Insert your dataset in the subfolder `/data/` and load it here. 
```{r read-data}
#| eval: !expr params$rerun
data <- read.csv(here::here("data", "WARND_stage2_c1234_trainingset_before_30min_v2024_07_16.csv"))

data <- data |> 
  filter(!is.na(counter))
```

Relevant EMA items and their reaction times: 
```{r ema-itemss}
ema_items <-  c("sad_d", "stressed_d", "overwhelm_d", "nervous_d", "ruminate_d", "irritable_d", "cheerful_d", "motivated_d", "relaxed_d")
rt_ema_items <- paste0("rt_", ema_items)
```


For later visualizations, we also load the results of different model-based approaches: 
```{r load-models}
lpa_res <- readRDS(here::here("output", "data_profiles_corrected.RDS"))
irt_res <- readRDS(here::here("output", "data_mixtureIRT.RDS"))
```

# Analysis

We first define a list of all individual indices and their relevant parameters. This depends on `sd_within_assessment` being the first, to have no implicit missingness when using `left_join()` later below.
```{r}
# Define the functions and their parameters
index_functions <- list(
    sd_within_assessment = list(
    func = calc_within_assessment_sd,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  mahalanobis = list(
    func = calc_indicator_mahalanobis,
    args = list(items = ema_items)
  ),
  robust_pca = list(
    func = calc_indicator_rob_PCA_orthogonal_distance,
    args = list(items = ema_items)
  ),
  psychometric_antonym = list(
    func = calc_psychometric_antonym_violations,
    args = list(
      items = ema_items,
      antonym_pair = c("stressed_d", "relaxed_d"),
      # cor_threshold = 0.45,
      antonym_maxvalue_threshold = 4
    )
  ),
  mean_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "mean")
  ),
  sd_response_times = list(
    func = calc_summary_response_times,
    args = list(items = rt_ema_items, summary = "sd")
  ),

  items_at_mode = list(
    func = calc_mode_percentage,
    args = list(
      items = ema_items,
      id_col = "external_id"
    )
  ),
  longstring = list(
    func = calc_longstring_counts, 
    args = list(
      items = ema_items, 
      rt_items = rt_ema_items
    )
  )
)


```


## Full Analysis Individual Indices
We split the full data set into multiple chunks for easier computation: 
```{r}
#| eval: !expr params$rerun
# get list of ids
unique_ids <- data |> 
  distinct(external_id) |> 
  pull(external_id)

# create chunk labels
num_per_group <- 50
num_groups <- ceiling(length(unique_ids) / num_per_group)
group_labels <- rep(1:num_groups, each = num_per_group)[seq_along(unique_ids)]

# map external_ids to groups
id_groups <- data.frame(external_id = unique_ids, group = group_labels)
data_grouped <- data |> left_join(id_groups, by = "external_id")

# Split into a list of dataframes, one for each group
data_subsets <- split(data_grouped, data_grouped$group)

df_indices_list <- list()

time_before <- Sys.time()


# Process each chunk
for (i in seq(num_groups)) {
  cat("Processing chunk", i, "of", num_groups, "\n")

  chunk_data <- data_subsets[[i]]

  # calculate and combine indices for the current chunk
  indices <- index_functions |>
    purrr::map(
      .f = function(x) {
        do.call(x$func, c(list(data = chunk_data), x$args))
      }
    )

  chunk_indices <- indices |>
    purrr::reduce(
      .f = function(x, y) {
        x |>
          left_join(y, by = c("external_id", "counter"))
      }
    ) |>
    dplyr::mutate(mode_count = if_else(is.nan(mode_count), NA, mode_count)) |>
    dplyr::rename(k_values = kValues)

  # convert proportions to absolute numbers
  chunk_indices <- chunk_indices |>
    mutate(longstring_count = as.integer(longstring_count * length(ema_items)),
           mode_count = as.integer(mode_count * length(ema_items)))

  
  df_indices_list[[i]] <- chunk_indices
}

time_diff <- Sys.time() - time_before
# ~30mins.

# Combine all chunk results
df_indices <- df_indices_list |>
  purrr::reduce(
    .f = function(x, y) {
      x |>
        bind_rows(y, .id = "chunk_id")
    }
  ) |> 
  select(-chunk_id) |> 
  arrange(external_id, counter)

# save results
saveRDS(df_indices, file = here::here("output", "df_indices.RDS"))
```

Read back in: 
```{r}
#| label: load-indices
df_indices <- readRDS(file = here::here("output", "df_indices.RDS"))
```


For the robust PCA, we used the default settings of the `rrcov` R package with a maximum number of nine principal components, corresponding to the number of items used.

# Multiverse Indices

Here, we investigate different settings to detect careless responding. These concern: 
1. Different cutoffs and settings for individual indices
2. Different combination rules for multiple indices

We define a grid of different cutoffs and settings. 

```{r}
#| label: specifications
sd_within_cutoff <- c(0.75, 1, 1.25)
average_response_time_cutoff <- c(1, 1.5, 2)
sd_response_time_cutoff <- c(0.4, 0.6, 1)
longstring_cutoff <- c(6, 7, 8)
mode_cutoff <- c(5, 6)
anto_cutoff <- c(0)


specification_grid <- expand.grid(
  sd_within_cutoff, 
  average_response_time_cutoff, 
  sd_response_time_cutoff,
  longstring_cutoff,
  mode_cutoff,
  anto_cutoff
)

colnames(specification_grid) <- 
  c(
  "sd_within_cutoff", 
  "average_response_time_cutoff", 
  "sd_response_time_cutoff", 
  "longstring_cutoff", 
  "mode_cutoff",
  "anto_cutoff"
)
```

We then compute the multiverse by flagging each index at each observation as being indicative of careless responding or not:
```{r}
#| label: flag-careless
#| eval: !expr params$rerun
careless_results_list <- purrr::pmap(specification_grid, function(sd_within_cutoff, average_response_time_cutoff,
                                                  sd_response_time_cutoff, longstring_cutoff, mode_cutoff, anto_cutoff) {
  cutoffs <- list(
    sd_within_cutoff = sd_within_cutoff,
    average_response_time_cutoff = average_response_time_cutoff,
    sd_response_time_cutoff = sd_response_time_cutoff,
    longstring_cutoff = longstring_cutoff,
    mode_cutoff = mode_cutoff,
    anto_cutoff = anto_cutoff
  )
  flag_careless(df_indices, cutoffs)
})

saveRDS(careless_results_list, file = here::here("output", "careless_results_list.RDS"))
```

Reload the dataframe:
```{r}
#| label: reload-careless
#| eval: !expr params$rerun
careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))
df_careless <- bind_rows(careless_results_list)
```


We can then apply different combination rules for multiple indices. For the manuscript, we focus on our "main" specification, which is:

    - Within-assessment variability: 1.25
    - Longstring: 7/9
    - Mode: 5/9
    - Average response time: 2s
    - SD response time: 1s
    
```{r multiple-hurdles}
#| eval: !expr params$rerun
# select df of main specification
careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))

main_id <- specification_grid |>
  mutate(id = row_number()) |> 
  filter(
    sd_within_cutoff == 1.25 &
      average_response_time_cutoff == 2 &
      sd_response_time_cutoff == 1 &
      longstring_cutoff == 7 & mode_cutoff == 5
  ) |> 
    pull(id)


main_df <- careless_results_list[[main_id]]

# apply multiple hurdle approach
flag_df <- main_df |>
  dplyr::select(external_id, counter, starts_with("flag")) |>
  rowwise() |>
  mutate(
    flag_sum = sum(
      flag_sd_within,
      flag_average_response_time,
      flag_sd_response_time,
      flag_longstring,
      flag_mode,
      flag_anto,
      na.rm = TRUE
    )
  ) 


saveRDS(flag_df, file = here::here("output", "flag_df.RDS"))
```


Additionally, for purpose of comparison, we can extract the results for the most lenient cutoffs: 
```{r}
#| eval: !expr params$rerun
lenient_id <- specification_grid |>
  mutate(id = row_number()) |> 
  filter(
    sd_within_cutoff == 0.75 &
      average_response_time_cutoff == 1 &
      sd_response_time_cutoff == 0.4 &
      longstring_cutoff == 8 & mode_cutoff == 6
  ) |> 
    pull(id)


lenient_df <- careless_results_list[[lenient_id]]

# apply multiple hurdle approach
lenient_flag_df <- lenient_df |>
  dplyr::select(external_id, counter, starts_with("flag")) |>
  rowwise() |>
  mutate(
    flag_sum = sum(
      flag_sd_within,
      flag_average_response_time,
      flag_sd_response_time,
      flag_longstring,
      flag_mode,
      flag_anto,
      na.rm = TRUE
    )
  ) 

lenient_flag_df |>
  knitr::kable()

saveRDS(lenient_flag_df, file = here::here("output", "lenient_flag_df.RDS"))
```


## Checking impact of excluding careless responses

To understand the impact of different cutoffs for careless respondings, we can investigate how certain properties of the data or model results change before and after the exclusion of responses marked as careless. Here, inspired by Jaso et al. (2021), we do so by investigating how correlations between two items change. We pick "nervous" and "overwhelmed", as we expect these to correlate highly in non-careless responses.


### Multiverse Correlations
We iterate through the different specifications and compute the sum of flagged responses. We then select responses flagged as careless responses, and compute the correlation between the two negative affect items. 
```{r}
#| eval: !expr params$rerun

# make the careless results list smaller and easier to work with
careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags
flag_list <- lapply(careless_list, function(x){
    x |> 
    rowwise() |> 
    dplyr::mutate(flag_sum = sum(flag_sd_within, flag_average_response_time, flag_sd_response_time, flag_longstring, flag_mode, flag_anto, na.rm = TRUE)) |> 
    dplyr::select(external_id, counter, flag_sum)
})

# remove unnecessary data attributes
flag_list <- lapply(flag_list, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})
saveRDS(flag_list, file = here::here("output", "flag_list.RDS"))
```

Re-read the flag dataframe and filter careless responses for each specification, then compute the correlation of the two items:
```{r}
#| eval: !expr params$rerun
# select careless responses and filter them in original data, then compute correlation
flag_list <- readRDS(here::here("output", "flag_list.RDS"))


# compute correlation for each specification
compute_correlations <- function(flag_list, data) {
 imap(flag_list, function(flag_df, i) {
   cat("Processing iteration", i, "of", length(flag_list), "\n")
   
   excluded_ids_1plus <- flag_df |> 
     filter(flag_sum >= 1) |> 
     select(external_id, counter)
   
   excluded_ids_2plus <- flag_df |> 
     filter(flag_sum >= 2) |> 
     select(external_id, counter)
   
   excluded_data_1plus <- data |> 
     inner_join(excluded_ids_1plus, by = c("external_id", "counter"))
   
   excluded_data_2plus <- data |> 
     inner_join(excluded_ids_2plus, by = c("external_id", "counter"))
   
   list(
     correlation_1plus = cor(excluded_data_1plus$nervous_d, excluded_data_1plus$overwhelm_d, use = "pairwise.complete.obs"),
     n_excluded_1plus = nrow(excluded_data_1plus),
     correlation_2plus = cor(excluded_data_2plus$nervous_d, excluded_data_2plus$overwhelm_d, use = "pairwise.complete.obs"),
     n_excluded_2plus = nrow(excluded_data_2plus)
   )
 })
}

excluded_correlations <- compute_correlations(flag_list, data)

# combine to dataframe
df_cors <- bind_rows(excluded_correlations)

# add specification information
df_cors_spec <- cbind(df_cors, specification_grid)

saveRDS(df_cors_spec, here::here("output", "df_cors.RDS"))
```



### Multiverse Similarity 
Instead of checking the correlation between affect items, we can instead check the overlap between carelessness flags based on the indices and the model-based indices. 

```{r}
#| eval: !expr params$rerun
flag_list <- readRDS(here::here("output", "flag_list.RDS"))

# apply custom function
jaccard_results <- compute_similarities(flag_list, lpa_res, irt_res)
names(jaccard_results) <- names(flag_list)

# format properly and add specification info
df_jaccard <- bind_rows(jaccard_results)
df_jaccard_spec <- cbind(df_jaccard, specification_grid)


saveRDS(df_jaccard_spec, file = here::here("output", "df_jaccard_spec.RDS"))
```


We then see which specifications lead to the highest similarities between indices and model-based classifications:
```{r}
#| eval: !expr params$rerun
#| label: multiverse-similarity

df_jaccard_spec <- readRDS(here::here("output", "df_jaccard_spec.RDS"))

df_jaccard_spec |> 
  arrange(desc(jaccard_similarity1_lpa)) |> 
  head(10)
```



#### Revision: Checking Similarity without Response Time Indices

For the first revision of the manuscript, we additionally removed the response time indices from the index flagging and the LPA. We recompute the similarities to see if they differ compared to the original analysis containing the response times. 

```{r}
#| eval: !expr params$rerun
# make the careless results list smaller and easier to work with
careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))

careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags WITHOUT response time indices
flag_list_no_rt <- lapply(careless_list, function(x) {
  x |>
    rowwise() |>
    dplyr::mutate(
      flag_sum = sum(
        flag_sd_within,
        # flag_average_response_time,
        # flag_sd_response_time,
        flag_longstring,
        flag_mode,
        flag_anto,
        na.rm = TRUE
      )
    ) |>
    dplyr::select(external_id, counter, flag_sum)
})

# remove unnecessary data attributes
flag_list_no_rt <- lapply(flag_list_no_rt, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})
saveRDS(flag_list_no_rt, file = here::here("output", "flag_list_no_rt.RDS"))

```

We then choose a new 'default' specification, which we find by maximizing the overlap with the LPA results without response time indices. This leads to slightly different cutoffs than in the original analysis:

```{r}
#| eval: !expr params$rerun
#| label: specification-no-rt

df_jaccard_spec_no_rt <- readRDS(here::here("output", "df_jaccard_spec_no_rt.RDS"))

df_jaccard_spec_no_rt |> 
  arrange(desc(jaccard_similarity1_lpa)) |> 
  # filter maximum similarity to LPA
  filter(jaccard_similarity1_lpa == max(jaccard_similarity1_lpa)) |> 
  select(!c(contains("response_time"))) |> 
  distinct(jaccard_similarity1_lpa, sd_within_cutoff, longstring_cutoff, mode_cutoff, anto_cutoff) |> 
  knitr::kable()

```

For the new default, we now have a cutoff of within-response SD of 0.75.


Also compute main and lenient specifications without response time indices. The lenient specification remains the same as before, as it already had the most lenient cutoffs, irrespective of the inclusion of response time indices.
```{r}
#| eval: !expr params$rerun
#| label: multiple-hurdles-no-rt

# select df of main specification
careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))

main_id <- specification_grid |>
  mutate(id = row_number()) |> 
  filter(
    sd_within_cutoff == 0.75 &
      longstring_cutoff == 7 & 
      mode_cutoff == 5 & 
      anto_cutoff == 0 &
      # need to select some arbitrary values for the RT cutoffs, as they are not used here
      average_response_time_cutoff == 2 &
      sd_response_time_cutoff == 1
  ) |> 
    pull(id)


main_df <- careless_results_list[[main_id]]

# apply multiple hurdle approach
flag_df_no_rt <- main_df |>
  dplyr::select(external_id, counter, starts_with("flag")) |>
  rowwise() |>
  mutate(
    flag_sum = sum(
      flag_sd_within,
      # flag_average_response_time,
      # flag_sd_response_time,
      flag_longstring,
      flag_mode,
      flag_anto,
      na.rm = TRUE
    )
  ) 

saveRDS(flag_df_no_rt, file = here::here("output", "flag_df_no_rt.RDS"))


# Lenient 
lenient_id <- specification_grid |>
  mutate(id = row_number()) |> 
  filter(
    sd_within_cutoff == 0.75 &
      average_response_time_cutoff == 1 &
      sd_response_time_cutoff == 0.4 &
      longstring_cutoff == 8 & mode_cutoff == 6
  ) |> 
    pull(id)


lenient_df <- careless_results_list[[lenient_id]]

# apply multiple hurdle approach
lenient_flag_df_no_rt <- lenient_df |>
  dplyr::select(external_id, counter, starts_with("flag")) |>
  rowwise() |>
  mutate(
    flag_sum = sum(
      flag_sd_within,
      # flag_average_response_time,
      # flag_sd_response_time,
      flag_longstring,
      flag_mode,
      flag_anto,
      na.rm = TRUE
    )
  ) 


saveRDS(lenient_flag_df_no_rt, file = here::here("output", "lenient_flag_df_no_rt.RDS"))
```



We can then check the similarities again, while also using the LPA results without RTs. Importantly, in the revision, the first profile of the LPA is the attentive one, whereas it was the second one in the original analysis. 

```{r}
#| eval: !expr params$rerun
#| label: similarity-no-rt

flag_list_no_rt <- readRDS(here::here("output", "flag_list_no_rt.RDS"))
lpa_res_no_rt <- readRDS(here::here("output", "data_profiles_without_time_indices.RDS"))

# apply custom function
jaccard_results_no_rt <- compute_similarities(flag_list_no_rt, lpa_res_no_rt, irt_res,
                                              lpa_attentive_profile = 1)

# format properly and add specification info
df_jaccard_no_rt <- bind_rows(jaccard_results_no_rt)
df_jaccard_spec_no_rt <- cbind(df_jaccard_no_rt, specification_grid)


saveRDS(df_jaccard_spec_no_rt, file = here::here("output", "df_jaccard_spec_no_rt.RDS"))

```




## Multiverse descriptives

Instead of computing correlations or similarities, we can simply calculate the number of responses flagged as careless for each combination of indices: 
```{r}
#| label: multiverse-careless-pct
#| eval: !expr params$rerun
careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags
flag_list <- lapply(careless_list, function(x){
    x |> 
    rowwise() |> 
    dplyr::mutate(flag_sum = sum(flag_sd_within, flag_average_response_time, flag_sd_response_time, flag_longstring, flag_mode, flag_anto, na.rm = TRUE)) |>
    mutate(flag_sum1 = ifelse(flag_sum == 0, 0, 1),
            flag_sum2 = ifelse(flag_sum <2, 0, 1))
})

# remove unnecessary data attributes
flag_list <- lapply(flag_list, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})

rm(careless_list)

# for each list entry, calculate careless responding proportions
# normed by the number of responses in our main df
flag_df <- readRDS(here::here("output", "flag_df.RDS"))

total_obs <- nrow(flag_df)
non_missing_obs <- flag_df |> 
  filter(!is.na(flag_sd_within)) |> 
  nrow()


flag_summary <- lapply(flag_list, function(x){
  x |> 
  ungroup() |> 
  select(!c(external_id, counter, flag_sum)) |> 
  summarize(across(everything(), 
                   ~sum(., na.rm = TRUE))) |> 
  mutate(across(everything(), 
                ~ round(./ non_missing_obs * 100, 3))) 
})

rm(flag_list)

# attach specification information
flag_multiverse <- do.call(rbind, flag_summary) |> 
  bind_cols(specification_grid)


saveRDS(flag_multiverse, here::here("output", "flag_multiverse.RDS"))
```


### Revision: Checking Multiverse Without Reaction Times

Again, we here conduct the same analysis as above, but without response time indices:

```{r}
#| label: multiverse-careless-pct-no-rct
#| eval: !expr params$rerun

careless_results_list <- readRDS(here::here("output", "careless_results_list.RDS"))
careless_list <- lapply(careless_results_list, function(x){
    x |> 
    dplyr::select(external_id, counter, starts_with("flag"))
})

rm(careless_results_list)

# compute careless responding flags
flag_list_no_rt <- lapply(careless_list, function(x) {
  x |>
    rowwise() |>
    dplyr::mutate(
      flag_sum = sum(
        flag_sd_within,
        # flag_average_response_time,
        # flag_sd_response_time,
        flag_longstring,
        flag_mode,
        flag_anto,
        na.rm = TRUE
      )
    ) |>
    mutate(flag_sum1 = ifelse(flag_sum == 0, 0, 1),
           flag_sum2 = ifelse(flag_sum < 2, 0, 1))
})

# remove unnecessary data attributes
flag_list_no_rt <- lapply(flag_list_no_rt, function(x){
  attr(x, "groups") <- NULL
  x <- ungroup(x)
  x
})

rm(careless_list)

# for each list entry, calculate careless responding proportions
# normed by the number of responses in our main df
flag_df_no_rt <- readRDS(here::here("output", "flag_df_no_rt.RDS"))

total_obs <- nrow(flag_df_no_rt)
non_missing_obs <- flag_df_no_rt |> 
  filter(!is.na(flag_sd_within)) |> 
  nrow()


flag_summary_no_rt <- lapply(flag_list_no_rt, function(x){
  x |> 
  ungroup() |> 
  select(!c(external_id, counter, flag_sum)) |> 
  summarize(across(everything(), 
                   ~sum(., na.rm = TRUE))) |> 
  mutate(across(everything(), 
                ~ round(./ non_missing_obs * 100, 3))) 
})

rm(flag_list_no_rt)

# attach specification information
flag_multiverse_no_rt <- do.call(rbind, flag_summary_no_rt) |> 
  bind_cols(specification_grid)


saveRDS(flag_multiverse_no_rt, here::here("output", "flag_multiverse_no_rt.RDS"))
```


# Session Info

```{r session-info}
pander::pander(sessionInfo())

```

