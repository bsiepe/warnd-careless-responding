---
title: "Online Supplement 2: Latent Profile Analysis"
subtitle: "For the Manuscript: Identifying Careless Responding in Ecological Momentary Assessment: Inconsistent Signals from Different Detection Methods in the WARN-D Data"
author: 

 - name: Leonie V.D.E. Vogelsmeier
   orcid: 0000-0002-1666-7112
   affiliations: Tilburg University
 - name: Gudrun Eisele
   orcid: 0000-0002-4466-3733
   affiliations: KU Leuven
 - name: Esther Ulitzsch
   orcid: 0000-0002-9267-8542
   affiliations: Centre for Educational Measurement, University of Oslo; Centre for Research on Equality in Education, University of Oslo


date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: true # run code?
---

# Background

In this script, we investigate careless responding using a latent profile analysis (LPA) as an alternative to the multiple-hurdle approach. The method is explained in the main manuscript. In brief, in the LPA, the indices serve as occasion-specific indicators of occasion-specific latent profiles that differ in scoring patterns. We have the following indices in our dataset:

-   `assessment_sd`: Within-assessment response SD

-   `mahalanobis_dist`: Mahalnobis distance

-   `robustpca_dist`: Robust principal component analysis (PCA)

-   `anto_violations`: Antonym violation count

-   `average_response_time`: Average response time per item

-   `sd_response_time`: SD of response times across items

-   `longstring_count`: Long string count

-   `mode_count`: Mode count

Please note that in the main script, we used percentages for the mode and long string indices to enhance readability. In this supplement, however, we maintain the raw counts to preserve a scaling that is not too different across indices and to facilitate the plotting of the unstandardized means.

While most indices can be assumed to be continuous and thus modeled using a a profile-specific Gaussian distribution, the antonym violation count, which takes values of either 0 or 1 in our analysis, requires a binomial distribution. In this tutorial, we use the `depmixS4` package because it allows for the specification of different distributions for each index. It is important to note that one may also use the `tidyLPA` (which builds upon the `mclust` package) if only using continuous indices. In this supplement, we provide only a tutorial for the `depmixS4` package, as it is less intuitive to use compared to the `tidyLPA` package. The `tidyLPA` package is very user-friendly and comes with a tutorial (<https://cran.r-project.org/web/packages/tidyLPA/vignettes/Introduction_to_tidyLPA.html>).

# Preliminaries

We first install and load all relevant packages.

```{r packages, warning = FALSE, message = FALSE}
if(!require(pacman)) install.packages("pacman")
pacman::p_load("dplyr", "tidyr", "here", "ggplot2", "depmixS4", "tidyLPA",
               "tidyverse","viridis")
```

# Data Preparation

This section outlines the relevant preprocessing steps for the analysis. The data characteristics are described in detail in the main manuscript. For the LPA, data preparation is limited to loading the data, selecting complete cases only, and ensuring that the long string and mode indices are treated as numerical variables as they were stored as integers. The dataset with the computed indices that we load here was obtained as described in the tutorial on indices).

```{r data-preparation}
# load the data with the indices
data <- readRDS("df_indices.RDS")

# retain only the data for which we have all indices available
data <- data[complete.cases(data),]

data <- data %>%
  mutate(
    longstring_count = as.numeric(longstring_count),
    mode_count = as.numeric(mode_count)
  )
```

# Analysis

## Data Visualization

We start by visualizing the indices to get an idea of the distribution and variance.

```{r visualize indices}
indices <- c("assessment_sd", "mahalanobis_dist", "robustpca_dist",
             "anto_violations", "average_response_time", "sd_response_time",
             "longstring_count", "mode_count")

# pivot the data to long format for ggplot
data_long <- dplyr::select(data, dplyr::all_of(indices)) %>%
  tidyr::pivot_longer(cols = everything(), 
                      names_to = "variable", 
                      values_to = "value")


# plot distributions using ggplot
ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "#31688E", 
                 color = "white", 
                 alpha = 0.8) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distributions of Indices",
       x = "Value", y = "Count")
```

For the anotonym violation, it is difficult to see if there is variance. Therefore, we explicitly check whether the variance for this index is zero and if not, how many antonym violations are present.

```{r check variance}
# check if there is zero-variance in any index
sapply(data["anto_violations"], function(x) var(x, na.rm = TRUE) == 0)

# check how many antonym violations are present
table(data$anto_violations)
```

All indices have some variability across observations.

## Standardization

Next, we standardize all variables except the antonym count, which must remain an integer due to model specification requirements. Standardization ensures that variables are on a comparable scale, preventing those with larger variances or units from disproportionately influencing the clustering results. Note that when interpreting the results, it is easier to work with unstandardized variables. Therefore, after fitting the model, we reverse the standardization. To enable this, it is important to store the original means and standard deviations.

```{r standardize}
# store means and sds
indices_means <- apply(data[,indices], 2, mean)
indices_sds <- apply(data[,indices], 2, sd)

# store original data
data_unstandardized <- data

#standardize data
data[,indices[-4]] <- lapply(data[,indices[-4]], 
                             function(x) (x - mean(x)) / sd(x))
```

## Defining and Running the Model

Now, we define a list of indices that will be provided as input for the `depmixS4` LPA. Note that the analysis has problems with the index `sd_response_time` in that it leads to an error (`Error in if (sum(fbo$gamma[, i]) > 0) { : missing value where TRUE/FALSE needed!`\[\`). Since the estimation of the LPA is based on an expectation maximization algorithm that uses random starting values (see Visser & Speekenbrink, 2010), we have tried the estimation with different starting values. Using several starting values, one reduces the risk of finding local optima (for more information on starting values, see, e.g., Vogelsmeier et al. 2023). However, regardless of the starting values, the error persist, possibly due to the low variance of the index. However, this is merely speculation. We have a limited understanding of the index's expected behavior under attentive and careless response behavior. Therefore, it would have been interesting to see how it relates to other indices in the LPA. However, to proceed, we have removed it from the analysis.

```{r list indices}
# provide index names as in data frame
listIndices <- list(
  assessment_sd ~ 1, 
  mahalanobis_dist ~ 1,
  robustpca_dist ~ 1,
  anto_violations ~ 1,
  average_response_time ~ 1,
  #sd_response_time ~ 1,
  longstring_count ~ 1,
  mode_count ~ 1
)
```

Next, we specify the distributional assumptions for each index (given the latent profile), which is also required input for the `depmixS4` LPA. We assume a normal distribution for all indices but the antonym violation count, for which we assume a binomial distribution.

```{r list distributions}
# provide distributions in the same order as the list of index names
listDist <- list( 
  gaussian("identity"),
  gaussian("identity"),
  gaussian("identity"),
  binomial(),
  gaussian("identity"),
  #gaussian("identity"),
  gaussian("identity"),
  gaussian("identity")
)
```

In the next step, we determine the number of profiles. We here consider 1 up to 7 states. We perform the analysis 20 times per model with 20 different random starting values. As mentioned above, this helps reduce the risk for the model to converge to a local maximum. For each LPA model, we select the solution with the highest log-likelihood among the 20 replications. Note that there is no specific recommendation for the number of starting values to use in LPA. However, if the log-likelihood values vary substantially across replications for the model(s) that you want to choose for further investigation, it may be advisable to increase the number of replications to obtain more stable results. Due to the random starting values, we first set a seed to ensure the results can be replicated. Additionally, we define an empty results list (`results` in the code) and an empty model fit list (`fit` in the code) as well as an empty log-likelihood vector (`loglik` in the code) in which we store all relevant output for the 20 analyses. We can determine how stable the results are by looking at the range of the log-likelihood values. Furthermore, we can extract the results for the solution with the best log-likelihood and continue with those.

```{r analysis depmix, eval = FALSE}
# set a seed because of the random starting values
set.seed(2020)

# determine the number of random starting values (replications; reps) 
# per model
reps <- 20

# determine storage lists
results <- list() # solutions
fit <- list() # fit per solution
logliks_all <- list() # logliks per solution

# explore as many profiles as we have indices (i.e., 7 without sd_time)
# balancing the detection of meaningful solutions with manageable 
# computation time; number may be increased if model fit improves 
# substantially for the highest number of profiles.
for(k in 1:7){ 
  #define the model
  results <- depmix(listIndices,
                data = data,
                nstates = k,
                family = listDist,
                # treating observations as independent
                ntimes = rep(1,nrow(data))) 
  # for iteration/model k, make an empty list for storing the solutions
  fit.r <- list()
  # for iteration/model k, make an vector for storing the loglik values
  ll.k <- c()
  for(r in 1:reps){
    fit.r[[r]] <- NA
    try(
      # fit the model
      fit.r[[r]] <- fit(results) 
    )
    # store loglik if the model provides one
    if(!is.na(fit.r[[r]])){ 
      ll.k[r] <- round(logLik(fit.r[[r]]),2)
    }else{
      ll.k[r] <- NA
    }
  }
  # loglik should be smaller than zero to be valid
  ll.k[ll.k>0] <- NA 
  # select model with largest loglik
  fit[[k]] <- fit.r[[which.max(ll.k)[1]]] 
  # store logliks to compare how stable solutions are
  logliks_all[[k]] <- ll.k 
}

# extract fit and convergence and store into objects
logliks <- NA
bics <- NA
messages <- NA

for(i in 1:7){
  bics[i] <-  BIC(fit[[i]])
  logliks[i] <- logLik(fit[[i]])
  messages[i] <- fit[[i]]@message
}
```

⚠ Note that a warning appears for models with more than one profile (`non-integer #successes in a binomial glm!`), probably due to internal EM weighting across states. The truly dichotomous data is then weighted. It is thus harmless in this context and can be ignored. We save the results with the following syntax (note that for large datasets, the model objects can become very large, and saving them may take more than just seconds):

```{r save results, eval = FALSE}
save(logliks_all, file = "logliks_all.RData")
save(fit, file = "fit.RData")
save(results, file = "results.RData")
save(bics, file = "bics.RData")
save(logliks, file = "logliks.RData")
save(messages, file = "messages.RData")
```

```{r load results, include = FALSE}
# not visible in quarto output; used to load the results without re-estimating them
load("logliks_all.RData")
load("fit.RData")
load("results.RData")
load("bics.RData")
load("logliks.RData")
load("messages.RData")
```

## Extracting the Results

Now we extract the model fit and convergence results and check which model(s) should be further inspected. We rely on the BIC because this is the only commonly used measure that the `depmixS4` packages provides.

```{r compare solutions}
# combine into a data frame
results <- data.frame(nprofiles = 1:7, 
                      bics = bics, 
                      messages = messages)

# filter out rows with non-convergence messages
results_filtered <- results[!grepl("likelihood decreased", 
                                   results$messages, 
                                   ignore.case = TRUE), ]

# sort by BIC in decreasing order
results_sorted <- results_filtered[order(results_filtered$bics), ]

# print sorted results
print(results_sorted)

# plotting the decrease in the BIC
plot(y = results_sorted$bics, x = results_sorted$nprofiles, 
     ylab = "BIC", xlab = " number of profiles")
```

It appears that only models with 1 to 4 profiles converged and that the 4-profile solution has the lowest BIC. The improvement in model fit levels off somewhat, but there is no clear "elbow" in the plot. We will inspect the two best-fitting models and select the final model based on interpretability. However, first we check whether the best solutions for the two models that we inspect were found multiple times. Note that we ignore more decimal differences.

```{r stability}
best_loglik_counts <- sapply(logliks_all, function(x) {
  x_clean <- round(x[!is.na(x)]) # remove NAs and round
  best_value <- max(x_clean) # find the largest loglik
  sum(x_clean == best_value) # count how often it appears
})

best_loglik_counts[c(3,4)]
```

For models 3 and 4, the best solution was indeed found multiple times. Therefore, we do not increase the number of runs with random starting values and move on to the parameter extraction.

Extracting the parameters is not straightforward. Although they can be viewed with the `summary()` function, we prefer to store them in an object to facilitate plotting and comparing the best-fitting solutions. A for-loop can be used to extract the parameters. However, it is necessary to adjust the number of profiles and the names of the `means` and `sds` objects accordingly. Additionally, special attention is needed for the variable modeled with a binomial distribution. Unlike continuous indicators, a binomial variable is represented by a single log-odds parameter (that will be transformed into a probability by us), not a mean and standard deviation. As a result, there will be only one parameter instead of two, which complicates the for-loop for parameter extraction. Therefore, it is important to verify that the extracted parameters align with those presented in the `summary()` output. Note that, to enhance visual inspection, we order the results based on the order in the main manuscript, starting with the response pattern indices, followed by the consistency indices, and finally the response time index (keep in mind that the second one, `sd_response_time`, was omitted).

```{r order indices}
custom_order <- c("longstring_count", "mode_count", "assessment_sd", 
                  "mahalanobis_dist", "robustpca_dist", 
                  "anto_violations", 
                  "average_response_time")
```

Additionally, after extracting the parameters, we sort the profile columns based on the long string index. This step may be skipped, but we show it because, if one were to re-estimate the solutions (maybe because one forgot to set a seed), label switching may occur (e.g., Profile 2 becomes Profile 1). By consistently ordering the columns according to a specific index, one can ensure that the results remain comparable across different estimation runs. We begin with the extraction of the **3-profile** model results.

```{r parameters 3 profiles}
# define the number of profiles
nprofiles <- 3

# extract the parameters from fit objects
parameters <- getpars(fit[[nprofiles]], which = "pars")

means_3profs <- matrix(NA, nrow = (length(indices)-1), ncol = nprofiles)
nignore <- nprofiles+
  nprofiles*nprofiles # these first parameters can be ignored
count_profile <- 0
# get the means
for(k in 1:nprofiles){
  count_var <- 0
  for(v in 1:7){
    means_3profs[v,k] <- 
      parameters[(nignore + 1 + count_var + count_profile)]
    if(v == 4){
      count_var <- count_var + 1 # since there is no sd for the binomial model
    }else{
      count_var <- count_var + 2
    }
  }
  count_profile = count_profile + 7 + 6 # 7 intercepts and 6 sd per profile
}

sds_3profs <- matrix(NA, nrow = (length(indices)-1), ncol = nprofiles)
nignore <- nprofiles+nprofiles*nprofiles
count_profile <- 0
# get the means
for(k in 1:nprofiles){
  count_var <- 0
  for(v in 1:7){
    if(v == 4){
      sds_3profs[v,k] <- 0 # no such parameter for the binomial model
      count_var <- count_var + 1
    }else{
      sds_3profs[v,k] <- 
      parameters[(nignore + 2 + count_var + count_profile)]
      count_var <- count_var + 2
    }

  }
  count_profile = count_profile + 7 + 6 # 7 intercepts and 6 sd per profile
}

# compute probabilities for the antonym violation
means_3profs[4,] <- plogis(means_3profs[4,])

# back-standardize the means of the continuous indices
means_3profs[-4,] <- means_3profs[-4,]*
  indices_sds[-c(4,6)]+
  indices_means[-c(4,6)]

# back-standardize the SDs of the continuous indices
sds_3profs <- sds_3profs[-c(4), ] * 
  indices_sds[-c(4, 6)]
sds_3profs <- rbind(sds_3profs[1:3, ], rep(0, 3), sds_3profs[4:6, ])

# give names
row.names(means_3profs) <- indices[-6]
colnames(means_3profs) <- paste("profile",1:3, sep = "")
row.names(sds_3profs) <- indices[-6]
colnames(sds_3profs) <- paste("profile",1:3, sep = "")


# reorder the rows using based on the order in the manuscript
means_3profs <- means_3profs[custom_order, ]
sds_3profs <- sds_3profs[custom_order, ]

# extract the profile sizes
probs_3profs <- parameters[1:nprofiles]

# reorder profiles by longstring_count
longstring_order_3profs <- order(means_3profs["longstring_count", ])

means_3profs_sorted <- means_3profs[, longstring_order_3profs]
sds_3profs_sorted <- sds_3profs[, longstring_order_3profs]
probs_3profs_sorted <- probs_3profs[longstring_order_3profs]

colnames(means_3profs_sorted) <- paste("profile",1:3, sep = "")
colnames(sds_3profs_sorted) <- paste("profile",1:3, sep = "")
```

Next, we extract the results for the **4-profile** model.

```{r parameters 4 profiles}
# define the number of profiles
nprofiles <- 4

# extract the parameters from fit objects
parameters <- getpars(fit[[nprofiles]], which = "pars")

means_4profs <- matrix(NA, nrow = (length(indices)-1), ncol = nprofiles)
nignore <- nprofiles+
  nprofiles*nprofiles # these first parameters can be ignored
count_profile <- 0
# get the means
for(k in 1:nprofiles){
  count_var <- 0
  for(v in 1:7){
    means_4profs[v,k] <- 
      parameters[(nignore + 1 + count_var + count_profile)]
    if(v == 4){
      count_var <- count_var + 1 # since there is no sd for the binomial model
    }else{
      count_var <- count_var + 2
    }
  }
  count_profile = count_profile + 7 + 6 # 7 intercepts and 6 sd per profile
}

sds_4profs <- matrix(NA, nrow = (length(indices)-1), ncol = nprofiles)
nignore <- nprofiles+nprofiles*nprofiles
count_profile <- 0
# get the means
for(k in 1:nprofiles){
  count_var <- 0
  for(v in 1:7){
    if(v == 4){
      sds_4profs[v,k] <- 0 # no such parameter for the binomial model
      count_var <- count_var + 1
    }else{
      sds_4profs[v,k] <- 
      parameters[(nignore + 2 + count_var + count_profile)]
      count_var <- count_var + 2
    }

  }
  count_profile = count_profile + 7 + 6 # 7 intercepts and 6 sd per profile
}

# compute probabilities for the antonym violation
means_4profs[4,] <- plogis(means_4profs[4,])

# back-standardize the means of the continuous indices
means_4profs[-4,] <- means_4profs[-4,]*
  indices_sds[-c(4,6)]+
  indices_means[-c(4,6)]

# back-standardize the SDs of the continuous indices
sds_4profs <- sds_4profs[-c(4), ] * 
  indices_sds[-c(4, 6)]
sds_4profs <- rbind(sds_4profs[1:3, ], rep(0, 4), sds_4profs[4:6, ])

# give names
row.names(means_4profs) <- indices[-6]
colnames(means_4profs) <- paste("profile",1:4, sep = "")
row.names(sds_4profs) <- indices[-6]
colnames(sds_4profs) <- paste("profile",1:4, sep = "")


# reorder the rows using based on the order in the manuscript
means_4profs <- means_4profs[custom_order, ]
sds_4profs <- sds_4profs[custom_order, ]

# extract the profile sizes
probs_4profs <- parameters[1:nprofiles]

# reorder profiles by longstring_count
longstring_order_4profs <- order(means_4profs["longstring_count", ])

means_4profs_sorted <- means_4profs[, longstring_order_4profs]
sds_4profs_sorted <- sds_4profs[, longstring_order_4profs]
probs_4profs_sorted <- probs_4profs[longstring_order_4profs]

colnames(means_4profs_sorted) <- paste("profile",1:4, sep = "")
colnames(sds_4profs_sorted) <- paste("profile",1:4, sep = "")
```

We now plot the results. Note that the profile proportions (rounded to two decimals) are stated in the headings.

### Results 3 Profiles

The results for the **3-profile** model are:

```{r 3 profiles, fig.width = 9, fig.height = 6}
# reshape the data for 3 profiles
input_long <- means_3profs_sorted %>%
  as.data.frame() %>%
  tibble::rownames_to_column("variable") %>%
  pivot_longer(cols = -variable, names_to = "profile", values_to = "mean") %>%
  mutate(profile = factor(profile, levels = colnames(means_3profs_sorted)),
         variable = factor(variable, levels = rownames(means_3profs_sorted)))

# plot
ggplot(input_long, aes(x = variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
    scale_color_manual(values = c("#31688E", "#35B779", "#FDE725")) +
  theme_minimal() +
  labs(title = paste("Profile Sizes in Proportions: ",
                    paste(paste("Profile", 1:3, ":", 
                                round(probs_3profs_sorted, 2)),
                          collapse = " | "),
                    sep = ""), x = "Index", y = "Mean") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")

```

```{r 3 profiles including sd, fig.width = 9, fig.height = 6}
plot_data <- data.frame(
  Variable = rownames(means_3profs_sorted),
  Mean = means_3profs_sorted,
  SD = sds_3profs_sorted
)

colnames(plot_data)[2:4] <- paste0("Mean.profile", 1:3)
colnames(plot_data)[5:7] <- paste0("SD.profile", 1:3)

# reshape both Means and SDs into long format
plot_long <- plot_data %>%
  pivot_longer(
    cols = starts_with("Mean.profile"),
    names_to = "profile",
    names_prefix = "Mean.profile",
    values_to = "mean"
  ) %>%
  left_join(
    plot_data %>%
      pivot_longer(
        cols = starts_with("SD.profile"),
        names_to = "profile_sd",
        names_prefix = "SD.profile",
        values_to = "SD"
      ),
    by = c("Variable", "profile" = "profile_sd")
  ) %>%
  mutate(
    profile = factor(profile, levels = c("1", "2", "3")),
    Variable = factor(Variable, levels = rownames(means_3profs_sorted))
  )

# plot the data
ggplot(plot_long, aes(x = Variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = mean - SD, ymax = mean + SD),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  scale_color_manual(values = c(
    "1" = "#31688E",
    "2" = "#35B779",
    "3" = "#FDE725"
  )) +
  scale_x_discrete(labels = c(
    "longstring_count" = "long string count", 
    "mode_count" = "items that fall at the mode", 
    "assessment_sd" = "within-assessment response SD", 
    "mahalanobis_dist" = "Mahalanobis distance",
    "robustpca_dist" = "robust PCA distance",
    "anto_violations" = "likelihood of antonym violation",
    "average_response_time" = "average time-per-item"
  )) +
  theme_minimal(base_size = 14) +
  labs(x = "Index", y = "Mean ± SD") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  guides(color = guide_legend(title = NULL))

```

Note that the antonym violation probabilities (with averages of 0.00, 0.02, and 0.03 for the three profiles, respectively) and assessment standard deviation (with averages of 1.87, 1.52, and 1.68 for the three profiles, respectively) do not show clear differences across profiles and are therefore not further used for interpretation.

-   [**Profile 1**]{style="color:#31688E;"} is the largest profile and is characterized by generally low indices and moderately short response times, suggesting attentive responding.

-   [**Profile 2**]{style="color:#35B779;"} is distinguished by very long response times. Furthermore, the profiles is characterized by relatively large Mahalanobis and robust PCA indices, indicating consistency violations, which may point towards careless responding.

-   [**Profile 3**]{style="color:#FDE725;"} is marked by relatively high long string and mode indices, indicating aberrant response patterns that we may interpret as careless responding.

### Results 4 Profiles

The results for the **4-profile** model are:

```{r 4 profiles, fig.width = 9, fig.height = 6}
# reshape the data for 4 profiles
input_long <- means_4profs_sorted %>%
  as.data.frame() %>%
  tibble::rownames_to_column("variable") %>%
  pivot_longer(cols = -variable, names_to = "profile", values_to = "mean") %>%
   mutate(profile = factor(profile, levels = colnames(means_4profs_sorted)),
         variable = factor(variable, levels = rownames(means_4profs_sorted)))

# plot
ggplot(input_long, aes(x = variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  scale_color_manual(values = c("#440154", "#31688E", "#35B779", "#FDE725")) +
  theme_minimal() +
  labs(title = paste("Profile Sizes in Proportions: ",
                    paste(paste("Profile", 1:4, ":", 
                                round(probs_4profs_sorted, 2)),
                          collapse = " | "),
                    sep = ""), x = "Index", y = "Mean") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")
```

```{r 4 profiles including sd, fig.width = 9, fig.height = 6}
plot_data <- data.frame(
  Variable = rownames(means_4profs_sorted),
  Mean = means_4profs_sorted,
  SD = sds_4profs_sorted
)

colnames(plot_data)[2:5] <- paste0("Mean.profile", 1:4)
colnames(plot_data)[6:9] <- paste0("SD.profile", 1:4)

# reshape both Means and SDs into long format
plot_long <- plot_data %>%
  pivot_longer(
    cols = starts_with("Mean.profile"),
    names_to = "profile",
    names_prefix = "Mean.profile",
    values_to = "mean"
  ) %>%
  left_join(
    plot_data %>%
      pivot_longer(
        cols = starts_with("SD.profile"),
        names_to = "profile_sd",
        names_prefix = "SD.profile",
        values_to = "SD"
      ),
    by = c("Variable", "profile" = "profile_sd")
  ) %>%
  mutate(
    profile = factor(profile, levels = c("1", "2", "3", "4")),
    Variable = factor(Variable, levels = rownames(means_4profs_sorted))
  )



# plot the data
ggplot(plot_long, aes(x = Variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = mean - SD, ymax = mean + SD),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  scale_color_manual(values = c(
    "1" = "#440154",
    "2" = "#31688E",
    "3" = "#35B779",
    "4" = "#FDE725"
  )) +
  scale_x_discrete(labels = c(
    "longstring_count" = "long string count", 
    "mode_count" = "items that fall at the mode", 
    "assessment_sd" = "within-assessment response SD", 
    "mahalanobis_dist" = "Mahalanobis distance",
    "robustpca_dist" = "robust PCA distance",
    "anto_violations" = "likelihood of antonym violation",
    "average_response_time" = "average time-per-item"
  )) +
  theme_minimal(base_size = 14) +
  labs(x = "Index", y = "Mean ± SD") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  guides(color = guide_legend(title = NULL))
```

Note that the antonym violation probabilities (with averages of 0.02, 0.04, 0.01, and 0.00 for the four profiles, respectively) and assessment standard deviation (with averages of 1.48, 1.69, 1.56, and 1.94 for the four profiles, respectively) again do not show clear differences across profiles and are thus not further considered. Furthermore, note that we use the same color for similar profiles across the two solutions.

-   [**Profile 1**]{style="color:#440154;"} is characterized by relatively large Mahalanobis and robust PCA indices, indicating consistency violations, a new profile pattern compared to the 3-profile solution that may be interpreted as careless responding.

-   [**Profile 2**]{style="color:#31688E;"} is the largest profile and is characterized by generally low indices and moderately short response times, suggesting attentive responding (overlapping with Profile 1 in the previous solution).

-   [**Profile 3**]{style="color:#35B779;"} is characterized by a large response time and thus seems to be similar to Profile 2 in the previous 3-profile solution. However, the profile is no longer characterized by large Mahalanobis and robust PCA indices (Profile 1 seems to capture observations with high consistency violations now). Instead, the large response time profile is now additionally marked by relatively high long string and mode indices, suggesting aberrant response patterns, hinting towards careless responding.

-   [**Profile 4**]{style="color:#FDE725;"} is marked by relatively high long string and mode indices (comparable to Profile 3 of this 4-profile solution, yet with a slightly higher mode index), again suggesting aberrant response patterns, which we may interpret as careless responding (overlapping with Profile 3 in the previous solution).

We thus conclude that adding a fourth profile ([**Profile 1**]{style="color:#440154;"}) is substantively interesting, as it potentially captures a third type of careless responding. Next, for the chosen 4-profile solution, we we will extract the modal profile assignments and attach them to the dataset. Since the profile order has changed when re-sorting the profiles by the long string index, we must apply the updated order when attaching the assignments.

```{r}
# attach the assignments to the dataset
data$LPA_profile <- fit[[4]]@posterior[,1]

# check the order that we used for plotting
longstring_order_4profs
```

```{r}
# profiles 1 and 2 have to be swapped
data$LPA_profile[data$LPA_profile == 1] <- 5# temporarily replace 1 with 5
data$LPA_profile[data$LPA_profile == 2] <- 1# replace 2 with 1
data$LPA_profile[data$LPA_profile == 5] <- 2# replace 5 with 2
```

```{r, save data with assignments, eval = FALSE, include = FALSE}
saveRDS(data[,c("external_id","counter","LPA_profile")], file = "data_profiles_corrected.RDS")
```

To verify that we did not make a mistake, we check if the final proportions align with the estimated profile sizes of the 4-Profile solution, keeping in mind that small differences may result from classification uncertainty because not all observations are assigned to the profiles with probabilities of 1 or 0.

```{r}
# comparing the two sets of probabilities
round(table(data$LPA_profile)/nrow(data),2)
round(probs_4profs_sorted,2)
```

The probabilities are indeed approximately the same. As is further discussed in the main manuscript, only 38 percent of the observations are categorized as attentive based on our interpretation of the profiles. It is important to note that responses labeled as aberrant inconsistent are defined only relative to the other observed patterns, which entails ambiguity. Given the large proportions of these supposedly careless profiles, it is quite possible that meaningful substantive variation in response patterns is being misclassified as careless responding under this approach

With this last step, we conclude the analysis. Note that one could apply Steps 2 and 3 exactly as in the mixture IRT tutorial to further investigate transitions between the profiles. The only thing that would change is the number of states, as this would be equal to 4 now. The posterior probabilities to compute the classification error in Step 2 could be obtained from the `fit[[4]]@posterior`. As the steps are exactly the same, and because the steps are not officially part of this analysis, we will not conduct them here.

```{r save profile plot, eval = FALSE, include = FALSE}
pdf("profiles_labels.pdf", width = 8, height = 6) 
ggplot(input_long, aes(x = variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  scale_color_manual(values = c("#440154", "#31688E", "#35B779", "#FDE725"),
                     labels = c("1 (inconsistent; 23%)", "2 (attentive; 38%)", "3 (large response time; 11%)", "4 (invariable; 27%)")) +
  scale_x_discrete(labels = c("longstring_count" = "long string count", 
                              "mode_count" = "items that fall at the mode", 
                              "assessment_sd" = "within-assessment response SD", 
                              "mahalanobis_dist" = "Mahalanobis distance",
                              "robustpca_dist" = "robust PCA distance",
                              "anto_violations" = "likelihood of antonym violation",
                              "average_response_time" = "average time-per-item")) + 
  theme_minimal(base_size = 14) +
  labs(x = "Index", y = "Mean") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")+
  guides(color = guide_legend(title = NULL))
dev.off()
```

```{r save profile plot with means and sd, eval = FALSE, include = FALSE}
pdf("profiles_mean_sd.pdf", width = 10, height = 7)

plot_data <- data.frame(
  Variable = rownames(means_4profs_sorted),
  Mean = means_4profs_sorted,
  SD = sds_4profs_sorted
)

colnames(plot_data)[2:5] <- paste0("Mean.profile", 1:4)
colnames(plot_data)[6:9] <- paste0("SD.profile", 1:4)


# reshape both Means and SDs into long format
# reshape both Means and SDs into long format
plot_long <- plot_data %>%
  pivot_longer(
    cols = starts_with("Mean.profile"),
    names_to = "profile",
    names_prefix = "Mean.profile",
    values_to = "mean"
  ) %>%
  left_join(
    plot_data %>%
      pivot_longer(
        cols = starts_with("SD.profile"),
        names_to = "profile_sd",
        names_prefix = "SD.profile",
        values_to = "SD"
      ),
    by = c("Variable", "profile" = "profile_sd")
  ) %>%
  mutate(
    profile = factor(profile, levels = c("1", "2", "3", "4")),
    Variable = factor(Variable, levels = rownames(means_4profs_sorted))
  )



# plot the data
ggplot(plot_long, aes(x = Variable, y = mean, color = profile)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = mean - SD, ymax = mean + SD),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  scale_color_manual(values = c(
    "1" = "#440154",
    "2" = "#31688E",
    "3" = "#35B779",
    "4" = "#FDE725"
  )) +
  scale_x_discrete(labels = c(
    "longstring_count" = "long string count", 
    "mode_count" = "items that fall at the mode", 
    "assessment_sd" = "within-assessment response SD", 
    "mahalanobis_dist" = "Mahalanobis distance",
    "robustpca_dist" = "robust PCA distance",
    "anto_violations" = "likelihood of antonym violation",
    "average_response_time" = "average time-per-item"
  )) +
  theme_minimal(base_size = 14) +
  labs(x = "Index", y = "Mean ± SD") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  guides(color = guide_legend(title = NULL))

dev.off()
```

# Additional Plotting: Distributions per Profile Using Mondal Assignments (let's discuss what we want to keep)

```{r}
data_with_profiles <- cbind(data[,"LPA_profile"], data_unstandardized)

data_with_profiles_1 <- subset(data_with_profiles, LPA_profile == 1)
data_with_profiles_2 <- subset(data_with_profiles, LPA_profile == 2)
data_with_profiles_3 <- subset(data_with_profiles, LPA_profile == 3)
data_with_profiles_4 <- subset(data_with_profiles, LPA_profile == 4)
```

## Distributions Profile 1

```{r}
data_long <- dplyr::select(data_with_profiles_1, dplyr::all_of(indices)) %>%
  tidyr::pivot_longer(cols = everything(), 
                      names_to = "variable", 
                      values_to = "value")
ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "#31688E", 
                 color = "white", 
                 alpha = 0.8) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distributions of Indices",
       x = "Value", y = "Count")

```

## Distributions Profile 2

```{r}
data_long <- dplyr::select(data_with_profiles_2, dplyr::all_of(indices)) %>%
  tidyr::pivot_longer(cols = everything(), 
                      names_to = "variable", 
                      values_to = "value")
ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "#31688E", 
                 color = "white", 
                 alpha = 0.8) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distributions of Indices",
       x = "Value", y = "Count")

```

## Distributions Profile 3

```{r}
data_long <- dplyr::select(data_with_profiles_3, dplyr::all_of(indices)) %>%
  tidyr::pivot_longer(cols = everything(), 
                      names_to = "variable", 
                      values_to = "value")
ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "#31688E", 
                 color = "white", 
                 alpha = 0.8) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distributions of Indices",
       x = "Value", y = "Count")

```

## Distributions Profile 4

```{r}
data_long <- dplyr::select(data_with_profiles_4, dplyr::all_of(indices)) %>%
  tidyr::pivot_longer(cols = everything(), 
                      names_to = "variable", 
                      values_to = "value")
ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "#31688E", 
                 color = "white", 
                 alpha = 0.8) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distributions of Indices",
       x = "Value", y = "Count")

```
